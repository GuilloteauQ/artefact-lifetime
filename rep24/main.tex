\documentclass[sigconf,natbib=false]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[ACM REP'24]{2024 ACM Conference on Reproducibility and Replicability}{June 18-20, 2024}{Rennes, France}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
\acmISBN{978-1-4503-XXXX-X/18/06}

\usepackage{tcolorbox}
\tcbuselibrary{theorems}
\usepackage{cleveref}
\newtcbtheorem[]{trap}{Pitfall}{colback=black!5,colframe=black!35,fonttitle=\bfseries}{th}
\newtcbtheorem[]{lesson}{Takeaway}{colback=black!5,colframe=black!35,fonttitle=\bfseries}{th}
\newcommand{\repro}{reproducibility}
\newcommand{\Repro}{Reproducibility}
\newcommand{\transpo}{\emph{Transposition}}
\newcommand{\flavour}{\emph{flavour}}
\newcommand{\flavours}{\emph{flavours}}
\newcommand{\ie}{\emph{i.e.,}}
\newcommand{\eg}{\emph{e.g.,}}
\newcommand{\nix}{\emph{Nix}}
\newcommand{\nixos}{\emph{NixOS}}
\newcommand{\nxc}{\emph{NixOS Compose}}
\newcommand{\enos}{\emph{EnOSlib}}
\newcommand{\grid}{\emph{Grid'5000}}
\newcommand{\kam}{\emph{Kameleon}}
\newcommand{\kad}{\emph{Kadeploy}}
\newcommand{\mel}{\emph{Melissa}}
\newcommand{\store}{\emph{Nix Store}}
\newcommand{\ad}{Artifact Description}
\newcommand{\aeval}{Artifact Evaluation}
\newcommand{\adae}{\ad/\aeval}
\newcommand{\todo}[1]{{\color{red}TODO: #1}}
\usepackage{hyperref}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{siunitx}
%\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{calc}
%\usepackage{tabularx}
\usepackage{fontawesome}
\usepackage[para,online,flushleft]{threeparttable}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{amsmath,amsfonts}
\usepackage{textcomp}

%\usepackage[backend=biber,style=trad-abbrv,firstinits=true]{biblatex}
\usepackage[
  datamodel=software,
  style=trad-abbrv,
  backend=biber
]{biblatex}
\addbibresource{references.bib}
\usepackage{software-biblatex}

\begin{document}

\title{State-of-practice of \ad s: Are~we~reproducible~yet?}

\author{Quentin Guilloteau}\author{Florina M. Ciorba}
\email{firstname.lastname@unibas.ch}
\affiliation{%
  \institution{University of Basel}
  \country{Switzerland}
}

% \renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
TODO
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%% PAPER STARTS HERE -----------------------------------------------------------------------------------------------

\section{Introduction}

plop \cite{Abril07}



\section{Text from phd thesis}

The scientific community as a whole has been traversing a \repro\ crisis for the last decade.
Computer science does not make an exception\ \cite{randallIrreproducibilityCrisisModern2018,baker500ScientistsLift2016}.

The \repro\ of the research work is essential to build robust knowledge, and it increases the reliability of results while limiting the number of methodology and analysis bias.
In 2015, Collberg et al.\ \cite{collberg_repeatability_2015} studied the \repro\ of 402 experimental papers published in *system* conferences and journals.
Each studied paper linked the source code used to perform their experiments. 
On those 402 papers, 46\% were not reproducible.
The main causes were:
\emph{(i)} the source code was actually not available,
\emph{(ii)} the code did not compile or did not run,
\emph{(iii)} the experiments required specific hardware

To highlight the reproducible research works, several publishers (like ACM or Springer) set up an artifact evaluation of a submission.
This peer review process of the experimental artifact can yield one or several badgers to the authors based on the level of \repro\ of their artifacts.

The term \repro\ is often used in a broad sense and gathers several concepts.
The definitions that we will use in the rest of this thesis are the ones defined by ACM for the validation of the submitted artifacts\ \cite{acm-badges}.
It is composed of three levels of \repro:

\begin{enumerate}
\item \emph{Repeatable}: the measures can be obtained again by the people at the origin of the work.
\item \emph{Reproducible}: the measures can be obtained again by people who do not belong to the original work and with the original artifact of the authors.
\item \emph{Replicable}: the measures can be obtained again by people who do not belong to the original work without the original artifact.
\end{enumerate}

The evaluation of artifact is a crucial point which allows guaranteeing the reproducibility of the experiments and the results.
However, this reproducibility is not sufficient.
Even if being able to reproduce an experiment is proof a scientific validation, the experiment and its environment are often too rigid to be extended by a third party, or even by the authors themselves. 
We believe that the notion that should be pushed by the community is the \emph{reproducibility with variation}.
By "variation" we mean that a third party is able to easily modify the environment of the experience to continue the research.
This means that the hardware and software environments as well as the experimental scripts must be correctly defined and can be modified easily.

This section focuses on the software environment.
For a global vision of the reproducibility problems, the readers might be interested in\ \cite{ivie2018reproducibility}.



Section\ \ref{sec:expe:repro:motiv} gives the context and the motivation of reproducibility for distributed systems.
Several frequently seen traps that might break the reproducibility of the software environment of an experiment are presented in Section\ \ref{sec:expe:repro:sw}.
Finally, Section\ \ref{sec:expe:repro:fpm} motivates the use of Functional Package Managers as a solution to most of the reproducibility problems of the software environments. 

\subsection{Context \& Motivation}\label{sec:expe:repro:motiv}

Imagine that in your childhood your grandma cooked a delicious chocolate cake, and that you now want to know how to do it yourself.
You even think that you can improve on it, and make it tastier!
You can try to reproduce the cake based on your far memories and culinary intuition, but the result might be disappointing...
Maybe your parents know the recipe!
But you might just get some fuzzy instructions.
Maybe your grandma just improvised the recipe!
Who knows?

You decide to go through the old stuff of your grandma's house.
By chance, you find an old recipe book.
You open it and from it falls a piece of paper with what looks like a cake recipe.
The handwriting matches the one from your grandma!

The recipe is clear, well detailed, and contains all the quantities for all the ingredients, the order of the different steps, the cooking time, etc.
You decide to follow the recipe literally, but the final result is not what you remembered...
Maybe you did not use the correct type of eggs, or that your oven is too different from your grandma's.
How to know?


An experiment without the environment in which it was executed makes it much more difficult to reproduce.
Indeed, side effects from the environment can happen and change the results of the experiment.
It is easy to forget to include in the software environment an element which impacts the performance of the experiment.
The performances, but also the results of a simple C application can depend on the compilation options\ \cite{stodden2018assessing} or also from the quantity of UNIX environment variables\ \cite{mytkowicz2009producing}.

Most of the current solutions in terms of "\repro" fall under the storage of artifacts (system images, containers, virtual machines) and replay of experiments\ \cite{rosendo2020e2clab, brammer2011paper, brinckman2019computing}.
Even if this is an important part of the \repro\ spectrum, nothing guarantees that the software environment can be re-built in the future, and thus nothing guarantees that the experiments can be re-run if the artifacts disappear. 


The step of artifact evaluation for the conferences is done soon after their initial construction.
It is thus very probable that the construction of the artifacts will be executed in a similar state of the packages mirrors (\texttt{apt}, \texttt{rpm}, etc.).
However, what will happen when someone will try to rebuild the environment in 1 year? 5 years? 10 years?
The objective of science is to base itself on robust works to continue to go forward (\emph{Stand on the shoulders of giants}).
This vision of "\textbf{short term reproducibility}" is a major obstacle to scientific progress and is in complete opposition to the science philosophy.


We think that the notion that should be highlighted is the concept of \textbf{variation}\ \cite{mercier2018considering, feitelson_repeatability_2015}.
This means allowing a third party to use the environment defined for an experiment in order to investigate another research idea.
An example of variation would be to change the MPI implementation used in an experiment (\eg\ MPICH instead of OpenMPI).
Being able to introduce such a variation is only possible if the initial environment is correctly defined.


\subsection{Frequent Traps of the \repro\ of software environments}\label{sec:expe:repro:sw}


\paragraph{Sharing the Environment}

An obvious way to fail the reproducibility of its experiments is not to share the used environments, or to share them in a perennial place.
Platforms such as Zenodo\ \cite{zenodo} or Software-Heritage\ \cite{swheritage} allow users to store artifacts (scripts, data, environments, etc.) permanently.


\paragraph{Knowledge of the environment}

In the case where the software environment contains only Python packages, freezing the dependencies with \texttt{pip} (\texttt{pip freeze}) is not enough.
\texttt{pip} only describes the Python environment, and ignores the system dependencies that numerous packages have. 
For example, freezing an environment containing the \texttt{zmq} Python package will not freeze the ZeroMQ system package installed on the system.  
Even if re-creating a Python environment from a \texttt{requirements.txt} is simple, installing a list of system packages with specific version is on the other hand much more complex.
Moreover, listing manually all the system packages by hand is error-prone, and the best way to forget a package.

Tools such as Spack\ \cite{gamblin_spack_2015} have a similar approach as \texttt{pip} but also for all the system packages and their dependencies.
It is possible to export the environment as a text file and to rebuild it on another machine.
However, the produced environment might not be completely identical.
Indeed, Spack uses applications that are already present on the machine to build the packages from the sources.
Especially, Spack assumes the presence of a C compiler on the system, and will use this C compiler to build the dependencies of the environment.
Hence, if two different machines have two different C compiler then the resulting environment could differ from the desired environment.
One clear advantage of Spack is the ease to introduce a variation in an environment through the command line.


Solutions such as Spack, \texttt{pip}, \texttt{conda} only focus on the software stack above the operating system.
However, results from experiments might depend on the version of the kernel, some drivers, etc.
Thus, it is important to capture \emph{entirely} the software stack.

\begin{trap}{Partially capturing the software environment of the experiment}{}
Experiments do not only depend on Python packages, but they can also depend on system packages, or even the version of the Linux kernel.
The software environment must be capture \emph{entirely}.
\end{trap}

Usually, the capture of the entire software stack goes through it encapsulation in a system image.
This image can then be deployed on machines to execute the experiments.
A way to generate a system image is to start from a base image, deploy this image, execute the commands required to set up the desired environment, and finally compress the image.
Platforms such as \grid\ \cite{grid5000} and Chamelon\ \cite{chameleon} propose to their users such tools (\texttt{tgz-g5k}\ \cite{tgz-g5k} and \texttt{cc-snapshot}\ \cite{cc-snapshot} respectively).
In the context of repeatability and replicability, if the image stays available, then this way to produce system images is adequate at best.
But, concerning the traceability of the build, one cannot verify the commands that have been used to generate the image, and thus relies completely on the documentation from the experimenter.
Moreover, such images are not adapted to be versioned with tools like \texttt{git} as they are in a binary format.
In the situation where the image is no longer available, re-building the exact image is complex and the precise introduction of variation is utopian.


\paragraph{Depending on an uncontrollable state}


One way to deploy a complete environment could be to rebuild it at each experiment.
EnOSlib\ \cite{cherrueau_enoslib_2022} is a Python library to manage distributed experiments.
It integrates a mechanism to install system packages in a programmatic fashion.
Users of EnOSlib can then, by executing their Python script, base their environment on a default image that they can modify as they need for their experiment.
This strategy of starting from scratch at every deployment has the advantage of making it harder to forget a dependency in the environment, as its absence would be detected at every execution.
However, it is still possible to make mistakes.
The \repro\ of the experiments using such solutions strongly depends on the base environment on which they are based.
These base environments are managed by administrators of the platforms and also have a finite lifetime, which raises the question of their permanence.  
Suppose that the administrators update the version of the base environment, what happens to the \repro\ of an experiment?

\begin{trap}{Forgetting to capture the software environment of the experiment/workflow manager}{}
Capturing the software environment in which the experiment/workflow manager is executed is as important and the environment of the experiment.
\end{trap}

A better approach to generate image is via *recipes*.
Those recipes, like \texttt{Dockerfile}s for Docker containers or Kameleon\ \cite{ruiz_reconstructable_2015} recipes for system images, are a sequence of commands to execute on a base image to generate the desired environment.
The text format of recipes make then much more suitable to version, share, and reconstruct them.
These base images have often several versions, which are identified by labels called \emph{tags}.
In the case of Docker, the tag of the latest version is often called \texttt{latest}.
Basing an environment on this tag breaks the traceability, and thus the reconstruction of the image itself.
Indeed, if a newer version is available at the time of a future rebuild of the environment, then the image will be based on this newer version and not the original version.
Another important question is to know if the base image and all the version are themselves reconstructive, and if it is not the case, what is the permanence of the platforms hosting those images?
For instance, the lifetime of \texttt{nvidia/cuda} Docker image is 6 months, after 6 months, the administrators delete the images.

\begin{trap}{Basing environments on non-reproducible images}{}
Be cautious with the longevity of the images on which you base your software environment.
By transitivity, if these images are not reproducible, so are yours.
\end{trap}

Another frequent problem is that the recipe performs an update of the mirror (\eg\ \texttt{apt get update}) before installing the required packages for the environment.
This has the bad property of breaking the \repro\ of the image.
Indeed, the image depends on the state of an external entity which is not controllable.
In order to be sure to use the exact same packages during a rebuild, a solution could be to use a *snapshot* of the mirror\footnote{Example for \texttt{debian}: \url{http://snapshot.debian.org/}}.
EnOSlib allows users to use this snapshot for the construction of the environment.
Concerning the introduction of variation, to base an image on a \emph{snapshot} is quite constraining and can make impossible the installation of specific version of packages, or can create conflicts with already installed packages.


When the recipe must download an object from the outside world, it is crucial to verify the content of the fetched object and compare it to the expected one.
In the case where the object is not checked, the environment depends on the state of the source of the object at the time of construction.
Hence, if the recipe calls `curl` to get a configuration file or a snapshot of a mirror for example, the recipe must also check the content of the files.
The usual way to do it is to compare the cryptographic hash of the downloaded object and the one of the expected object. 


A similar problem arises when the recipe downloads a package via `git` and build it from source.
In this case, it is paramount to correctly set the commit used in the recipe of the image.
Indeed, not knowing the commit used in the original image leads to having a dependence to the latest commit of the repository on the main branch.
Setting the commit used allows to know exactly the sources used, and simplifies the *controlled* introduction of variation in the environment (by changing commit for example). 

\begin{trap}{No checking the content of downloaded objects}{}
Every object coming from the outside of the environment must be examined to be sure that it contains the expected content. 
It is more important that the image fails to build if the content differs from the expected one, rather than the image silently builds with a different content.
\end{trap}

\subsection{Conclusion}

The computer science community starts to get interested in the problems of \repro\ of experiments.
However, the problems of \repro\ at the software level are not truly understood.
Setting up reproducible experiments is *extremely* complex.
The management of the software environment illustrate one facet of this complexity.
The usual tools (`pip`, `spack`, `docker`, etc.) do not answer the \repro\ problems without a huge effort by the experimenters, and only allow a \emph{short-term \repro}.
The *graal* of \repro\ is the precise introduction of variation in a third party defined environment.
This need for variation allows scientists to use solid contributions to continue research.
Even if there are no perfect solution yet, the need for a change of practice concerning \repro\ is needed.

%
% /////////////////////////////////////////////////////////////////////////////////////////////////////
\section{Context}

\todo{}

\subsection{Tools}

Prova \cite{guerrera2019reproducible}
BenchOpt \cite{moreau2022benchopt}
UMLAUT \cite{umlaut}
Benchmarking Crimes \cite{van2018benchmarking}
Kheops \cite{rosendo2023kheops}

Cascad \cite{perignon2019certify}

\subsection{Journals}

IPOL \cite{colom2015ipol}
Rescience \cite{rougier2019rescience}
JOSS \cite{smith2018journal}
Papers with code \cite{paperswithcode}

\subsection{Surveys}

Propogation of ML research  \cite{kang2023papers}
Survey Sascha \cite{hunold2015survey}
Survey 4R \cite{hernandez2023repeatability}
Quality indicator \cite{castell2024towards}





%
% /////////////////////////////////////////////////////////////////////////////////////////////////////
\section{Recommendations from Conferences}

\todo{look at the recommendation of the conferences for the \adae\ and summarize}

%
% /////////////////////////////////////////////////////////////////////////////////////////////////////
\section{State-of-practice and limits of \ad}\label{sec:sop}

\input{tables/summary_conferences}

In this Section, we surveyed \todo{N} papers from the top conferences in distributed systems and system in 2023.
All of the considered conferences had an \adae\ process for the \emph{accepted} papers.
This \adae\ process usually consists of a first phase where the authors write an \ad\ as an appendix of the paper to show how to get and use the artifact, how to install the dependencies, what are the different experiments and their estimated time, etc.
This \ad\ section is usually a page long (in a double column layout).
The most popular practice is for authors to give a link to a more detailed description of the artifact.

Table \ref{tab:table:paper_confs} summarizes the number of papers evaluated for each conferences, as well as the number of artifact sections.


\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figs/does_paper_with_badge_has_artifact_section}
  \caption{}\label{fig:}
\end{figure}

A surprising realisation is that not all the papers with the "Open" badge have the artifact section in the proceeding version.
We suspect that either the author forgot, or refused, to include the section in the final version of the paper.
We believe that \textbf{the \ad\ is for the artifact reviewer, but also a future reader of the paper.}
Excluding this section of the final version should not grant the "Open" badge.

\subsection{Methodology}

We selected conferences of 2023 with an \ad\ process, and looked at their proceedings.
From the proceedings, we surveyed all the papers and, for each paper, noted:

\begin{itemize}
  \item How many reproducibility badges, and which, did the paper received
  \item If the paper has an \ad\ section
  \item If the paper shares the URL of the source code (it does not have to be in the \ad), and if the URL is still valid
  \item How the experiments were performed (\eg\ local machines, shared testbeds, proprietary machines, supercomputer, simulation, etc.)
  \item How was the source code shared (\texttt{git}, Zenodo, Software-Heritage, or combination of solutions)
  \item If the source code has been shared with \texttt{git}, we record the number of commits, and check if a precise commit has been specified by the authors
  \item How the software environment was shared
\end{itemize}


In the following of this Section, we study the \ad s on four characteristics: how is the source code shared (Section \ref{sec:sop:src}), where are the experiments executed (Section \ref{sec:sop:expe}), how is the software environment described and shared (Section \ref{sec:sop:sw}), and finally the worflow of experiments (Section \ref{sec:sop:workflow}).

\subsection{Source code}\label{sec:sop:src}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figs/how_repo_shared}
  \caption{}\label{fig:how_repo_shared}
\end{figure}

\todo{update order based on new papers}

The \ad\ gives a link to where to download the artifact.
Figure \ref{fig:how_repo_shared} shows how the code of the artifact was shared in the \ad.
The majority of papers simply gives the URL of a \texttt{git} repository.
Some paper shared their code with Zenodo\ \cite{zenodo} or Figshare\ \cite{figshare}, and some shared with both a \texttt{git} URL and Zenodo.
A minority used Software-Heritage\ \cite{swheritage}.

Some authors used \texttt{anonymous.4open.science}\ \cite{anonymous_github} which allows users to share with the reviewers an anonymous copy of a public Github repository.
This is particularly useful for double blind reviews.
However, for all the papers surveyed, all the links to this service were dead, and there was no way to find the original \texttt{git} reprository.
We believe that the links simply expired, which is problematic for the usage of future researchers.
Moreover, as long as the \aeval\ process does not count in the accept/reject decision, having a double blind review for the \adae\ only limits the ability of the reviewers and authors to communicate.
If the results of the \aeval\ will be taken into account for the decision, then the community should investigate ways to perform the \aeval\ in a double blind fashion.
An easy solution would be to use tools such as \texttt{anonymous.4open.science} for the review, and then substitute in the camera ready version of the paper the URL for a more persistant one.

When authors shared only with a \texttt{git} URL, they almost never mention the commit used for their experiments (see Figure \ref{fig:was_commit_fixed}).
Such a solution might be satisfactory for the \aeval\ as the delay between the submition of the paper and the evalation of its artifact is short enough to keep the code in the same, or a similar state.
However, for future researchers aiming to build upon these artifacts, it is nearly impossible to know which version of the code was used.
Moreover source code hosted on forges might not be available forever.
Authors could decide to delete or rename their repository, invalidating the URL given in the \ad.
Worse, the entire source forge might need to close. \todo{ref about gforge inria?}
\todo{a word on publiware?}

\todo{link to Section 3}
One solution proposed by the reproducibility guidelines of the conferences is to archive the code via Zenodo or Figshare, and then reference the DOI generated by these archive websites in the \ad.
This has the advantage of freezing the source code in a permanent fashion, and allow future usage of the code.
However, storing source code on Zenodo has a simple drawback: there is no code exploration in the browser.
From the point of view of future researchers, having to download locally potentially large archives from Zenodo to be able to explore a few files of the source code is cumbersome and introduces friction.
A better solution woulbe be to explore the artifact via a simple web UI.
Archiving can also break the link between the original \texttt{git} URL if not archived correctly.
Zenodo has an integration with Github\ \cite{github_zenodo} which allows to archive releases of a reprository.
This is why some authors share both the \texttt{git} URL and a Zenodo archive.
If the link between the repository and the Zenodo archive breaks (\eg\ \texttt{git} repository disappears), then future researchers are left with a single commit of the source code, and all the history of the project, which contributes to the understanding and extensibility of the project, is lost.
Some artifact shared via Zenodo are actually an archive of a \texttt{git} repository and include the \texttt{.git} folder.
\todo{more.}
Zenodo and Figshare are more adapted to archive datasets and binaries, not source code.

A more appropriate solution is to use Software-Heritage\ \cite{swheritage, di2017software}.
Similarly to Zenodo, it offers a permanent storage of source code, with the same interface as usual source forges (\eg\ Github, Gitlab, etc.).
It also refers the orignal source, so that future researcher can see it if still available.
Authors give an unique identifier for the revision. 
\todo{share the swhid of this repo}

\todo{what we want: history + check new version + web UI}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figs/was_commit_fixed}
  \caption{}\label{fig:was_commit_fixed}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{./figs/number_commits_repo.pdf}
  \caption{Cumlative distribution function of the number of commits in the repositories shared in the \ad s}\label{fig:number_commits_repo}
\end{figure}

One thing that we observed during this survey is the very low number of commits in the repositories linked in the \ad s when shared with \texttt{git} (\ie\ \texttt{git}, \texttt{git+zenodo}, \texttt{git+figshare} on Figure \ref{fig:how_repo_shared}).
Figure \ref{fig:number_commits_repo} shows the cumulative distribution function of the number of commits in the repositories shared in the \ad s when the source code was shared via \texttt{git}.
\todo{update this final data: We can see that more than a third have less than 10 commits.}
These repositories appear to be a "dump" of the source code with some extra commits for documentation.
\todo{dump from where?}
Such practices do not allow reviewers and future researchers to explore the "true" history of the project.
The same comments as for a standalone Zenodo archive apply here.
It also makes it suspicious about the good practices of the authors in terms of experimentations (traceability).
This problem might come from the fact that the \aeval\ process, and reproducibility, is only a second though for authors.

\begin{lesson}{Sharing source code}{}
  A shared \texttt{git} URL might not be available in the future.
  Archiving via Zenodo is better but introduce friction for future exploration.
  Using Software-Heritage appears to be the best available solution to permanently share source code.
\end{lesson}

\todo{cloud/google drive}

\subsection{Experimental setup}\label{sec:sop:expe}

One important point about reproducing experiments is the hardware used.
Any experiment that exhibts a particular behavior or performance evaluation should have a description of the hardware. 

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{./figs/experimental_setup.pdf}
  \caption{}\label{fig:experimental_setup}
\end{figure}

Figure \ref{fig:experimental_setup} depicts on which platforms the experiments of the considered papers were executed.
Most of the experimental platforms were local machines ("homemade"), but the description of the machine (\ie\ CPU, GPU, disk, etc.) were given.
This still makes it difficult for the reviewers to find the exact hardware, or the closest they can.
In some \ad s, we observed that authors were gave access to their local machines by giving the IP address and the password to connect.


A better solution would be to use open and shares platforms, also called \emph{testbeds}.
Chameleon\ \cite{chameleon}, Grid'5000\ \cite{grid5000}, or CloudLab\ \cite{cloudlab} are example of such testbeds.
\todo{more \cite{nussbaum2017testbeds}}

\todo{SC24 will use Chameleon for \aeval}

In practice testbeds are not used a lot.
However, proprietary solutions are.
Authors rely on platforms such as Amazon Web Services, Microsoft Azure , Google Cloud, etc.
Even if this allows the reviewers to get access to probably similar machines (in the short term), it hides the experiments, and thus their reproducibility, behind a paywall, which goes against the principle of Open Science.
Using proprietary platforms also raises the question of who should pay to reproduce the results of the authors?
Some authors using such platforms did write in their \ad\ the estimated monetary cost of rerunning the experiments.

Similarly, some papers (in particular for the SuperComputing conference (\texttt{sc23})) were using supercomputer to run their experiments.
While supercomputers are at the bleeding edge of technology, having access to such system is restrictive and can take several weeks or month before getting accepted.
\todo{berk}

\begin{lesson}{Experimenal setup}{}
  The majority of \ad s use machines that are difficult to get access to (local, supercomputer, or proprietary). 
  The testbeds are under representated in the state-of-practice, but appear to be the the way forward \cite{nussbaum2017testbeds} (\todo{berk}).
\end{lesson}

\subsection{Software environment}\label{sec:sop:sw}

\begin{figure*}
  \centering
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/how_packaged.pdf}
  \caption{}\label{fig:techno}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figs/image_cache_bin.pdf}
    \caption{}\label{fig:cache_bin}
  \end{subfigure}
  \caption{\todo{}}\label{fig:techo_cache}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=0.95\textwidth]{figs/sw_envs}
  \caption{}\label{fig:sw_envs}
\end{figure*}

After downloading the correct version of the code on the correct platform, reviewers must set up the correct software environment to execute the experiments.
Figure \ref{fig:sw_envs} shows the different techniques used to describe and share the software environment in the \ad s.
Note that the authors can use \emph{several} of these techniques in their \ad.

In the following of this Section, we go through the methods observed to share the artifact software environment, and discuss their reproducibility.

\subsubsection{Images}

Figure \ref{fig:techno} shows which tools were used to capture the software environment of the experiments.
Most of the \ad\ do not use any particular tool.
But some use virtualization tools such as container and virtual machines.

\todo{also chameleon images}

Usually, the capture of the entire software stack goes through it encapsulation in a system image.
This image can then be deployed on machines to execute the experiments.
A way to generate a system image is to start from a base image, deploy this image, execute the commands required to set up the desired environment, and finally compress the image.
Platforms such as \grid\ \cite{grid5000} and Chamelon\ \cite{chameleon} propose to their users such tools (\texttt{tgz-g5k}\ \cite{tgz-g5k} and \texttt{cc-snapshot}\ \cite{cc-snapshot} respectively).
In the context of repeatability and replicability, if the image stays available, then this way to produce system images is adequate at best.
But, concerning the traceability of the build, one cannot verify the commands that have been used to generate the image, and thus relies completely on the documentation from the experimenter.
Moreover, such images are not adapted to be versioned with tools like \texttt{git} as they are in a binary format.
In the situation where the image is no longer available, re-building the exact image is complex and the precise introduction of variation is utopian.

Figure \ref{fig:cache_bin} depicts the state of the images for the \aeval.
We observed that the majority of the authors who use an image make it available in a binary cache such as DockerHub. 
However, DockerHub does not offer permanent storage of the images, or the authors could push another image on top of the previous one, thus losing the traceability for the experiments.
Another solution is to archive the image in a long-term binary cache such as Zenodo.
However, it is not often done by the authors.

A better approach to generate and share images is via \emph{recipes}.
Those recipes, like \texttt{Dockerfile}s for Docker containers or Kameleon\ \cite{ruiz_reconstructable_2015} recipes for system images, are a sequence of commands to execute on a base image to generate the desired environment.
The text format of recipes make then much more suitable to version, share, and reconstruct them.
These base images have often several versions, which are identified by labels called \emph{tags}.
In the case of Docker, the tag of the latest version is often called \texttt{latest}.
Basing an environment on this tag breaks the traceability, and thus the reconstruction of the image itself.
Indeed, if a newer version is available at the time of a future rebuild of the environment, then the image will be based on this newer version and not the original version.
Another important question is to know if the base image and all the version are themselves reconstructive, and if it is not the case, what is the permanence of the platforms hosting those images?
For instance, the lifetime of \texttt{nvidia/cuda} Docker image is 6 months, after 6 months, the administrators delete the images\ \cite{nvidia_cuda_lifetime}.
However, in Figure \ref{fig:cache_bin}, we see that more than \todo{half} of the \ad\ using an image do not share the recipe, or we could not find the recipe, to inspect or rebuild the image.
This means that if the image is not in a binary cache, then it is impossible to rebuild it.


\subsubsection{List of package versions}\label{sec:sop:sw:list}

One of the popular approach to share the software environment is to simply list the dependencies of the artifact.
We observed several levels to such this listing approach.
The first level is to only give the name of the dependencies.
Then, authors can give a minimum version to use (\eg\ \texttt{gcc >= 10.0.0} \todo{check if makes sense}).
Finally, the most popular approach is to give for all the dependencies the version used.

Listing all the dependencies by hand raises several important questions.
Are actually \emph{all} the dependencies listed?
How to get another system in the same state?
What about the dependencies of the dependencies, etc?

\todo{more?}

\begin{lesson}{Listing dependencies}{}
  Simply listing the used packages is not enough to regenerate the correct software environment.
\end{lesson}

\subsubsection{Package managers installation commands}

Another popular way to describe the software environment is to list installation commands to the package manager (\eg\ \texttt{apt}, \texttt{yum}).
\todo{a word on what is a package manager}
Most of the time these commands look like:

\begin{verbatim}
sudo apt-get update
sudo apt-get install packageA packageB
\end{verbatim}

The question that arises is: which is the version of the installed packages?
Indeed, by calling \texttt{apt-get update} (or equivalent for the others package managers), now the environment depends on the state of the mirror of the package manager at the time the author did the experiments.
In the case of the \aeval, the mirror might not change "too much" between the time of the experiments and the review.
However, there is a very low probability that 5 or 10 years in the future, the mirror will be in the same state, and the installed versions will be the exact same as for the experiments of the authors.
This approach also implicitely defines a dependency on the operating system that needs to be used.

There are "workarounds" to be sure that the installed packages via classical package managers will be the expected ones. 
One of them is to use a \emph{snapshot} of the mirror\footnote{Example for \texttt{debian}: \url{http://snapshot.debian.org/}}.
These snapshots are a dump of the mirror at a given time and users can then install packages from these snapshots throught the usual interface of the package manager.
However, even using snapshots can cause reproducibility issues.
Indeed, what if the package installed from the snapshot creates a conflict with a package already installed on the system?
This is especially an issue for Filesystem Hierachy Standard (FHS) based systems (\eg\ Debian-based distributions) where all the binaries and libraries are stored into \texttt{/usr/bin} and \texttt{/usr/lib}.
For example, what happen to the already installed packages if the artifact requires to install an old version of the \texttt{glibc}? 

\todo{a word on variation}


\begin{lesson}{Classical package managers}{}
  Installing dependencies via classical package managers (\eg\ \texttt{apt}, \texttt{yum}) creates a dependency on a uncontrollable state: the state of the mirror of the package manager.
  Freezing the state of the mirror introduces new problems of compatibility with the underlying system and disables any introduction of variation.
\end{lesson}

\subsubsection{\texttt{pip} and \texttt{conda}}

In the case where the software environment contains only Python packages, freezing the dependencies with \texttt{pip} (\texttt{pip freeze}) is not enough.
\texttt{pip} only describes the Python environment, and ignores the system dependencies that numerous packages have. 
For example, freezing an environment containing the \texttt{zmq} Python package will not freeze the ZeroMQ system package installed on the system.  
Even if re-creating a Python environment from a \texttt{requirements.txt} is simple, installing a list of system packages with specific version is on the other hand much more complex.

In the best case, the repository include \texttt{requirements.txt} that list all the Python dependencies with the \emph{exact} versions.
However, in practice, we observed the same issues as presented in Section \ref{sec:sop:sw:list} where authors provide a list of dependencies without version, or with a loose version.

\todo{more on pip, pythonpath, conda}

\subsubsection{Downloading from the outside world}

A common practice when needed to install a package that is not available through the classical package managers, is to install it from source.
For this, authors indicate in the \ad\ how to download the dependency, and how to build it.
However, when cloning a \texttt{git} repository or downloading an archive via \texttt{wget}/\texttt{curl}, a common error is to not specify the commit to use.
If no commit is specified, \texttt{git} will use the latest commit of the main branch, which could be completely different at the moment of the artifact review and in 10 years.

Moreover, the downloaded \texttt{git} repository could disappear in the future, and thus cloning from Software-Heritage would be more robust than cloning from a forge (\eg\ Github, Gitlab).

Another important point is to check that the downloaded object is indeed the expected one.
This can be done by checking the cryptographic hash of the downloaded object and compare it to the expected one.
Among the surveyed papers, we observed this practice only once.

\begin{lesson}{Content of downloaded objects}{}
Every object coming from the outside of the environment must be examined to be sure that it contains the expected content. 
It is more important that the building of the environment fails if the content differs from the expected one, rather than the environment silently building with a different content.
\end{lesson}

\subsubsection{Modules}

A popular way to manage a software environment on HPC systems is through \emph{Modules} \cite{modules}.
Modules allow users to change their environment by "loading" and "unloading" packages, and allow to simply manage different versions of applications. 
Under the hood, modules change the \texttt{\$PATH} environment variables.
Modules are however mainly maintained by the administrators of the system, and are system-specific (\eg\ compile MPI with special optimizations for the underlying system).
Thus sharing a module-based environment between two systems might be impossible.
Moreover, as the \texttt{modulefiles} are managed by the administrators, they do not have an infinite lifetime, and might be unavailable in the future.
Modules are also helpful for administrators to limit the applications that can be run by users.
Modules can be generated with Easybuild \todo{ref + more}

\subsubsection{Spack}

Tools such as Spack\ \cite{gamblin_spack_2015} have a similar approach as \texttt{pip} but also for all the system packages and their dependencies.
It is possible to export the environment as a text file and to rebuild it on another machine.
However, the produced environment might not be completely identical.
Indeed, Spack uses applications that are already present on the machine to build the packages from the sources.
Especially, Spack assumes the presence of a C compiler on the system, and will use this C compiler to build the dependencies of the environment.
Hence, if two different machines have two different C compiler then the resulting environment could differ from the desired environment.
One clear advantage of Spack is the ease to introduce a variation in an environment through the command line.
Spack can also be run as a non privileged user and does not require the approval of the system administrators.
Spack will download and build dependencies into a folder located in the user's \texttt{HOME}.
However, a drawback on HPC system is that this directory consumes a lot of storage quota and inodes.

\subsubsection{Vendoring}

One way to make sure to use the correct dependencies is to "vendor" them.
This means having a copy of the dependencies source code which is build from source.
However, this approach has its limits has it does not capture all the dependencies (\eg\ C compiler).

\subsubsection{Nix (Functional package managers)}

Tools such as Nix\ \cite{dolstra_nix_2004} or Guix\ \cite{courtes_functional_2013} fix most of the problems described in the previous sections.
Nix and Guix share the similar concepts, in the following we will focus on Nix.

Nix is a pure functional package manager for the reproducibility of the packages.
A Nix package is defined as a function where the dependencies of the packages are the inputs of the function, the body of the function contains the instructions to build the package.
The building of the packages is done in a \emph{sandbox} which guarantees the build in a strict and controlled environment.
First, the sources are fetched, and the content verified by Nix.
If the hash of the sources differs from the expected hash, Nix stops the building of the package and yields an error.
Nix fetches also the dependencies and recursively.
The build commands are then executed in the sandbox with the environment defined by the user.
At this stage, no network access or access to the file system is possible.


Nix can generate environments that can be assimilated as multi-languages \texttt{virtualenv}s.
But it can also create containers images (Docker, Singularity, LXC, etc.), virtual machines, or full system images.
The process of building an image with classical tools (\texttt{Dockerfile}, Kameleon recipe, etc.) is often iterative and arduous.
Defining an image with Nix is done in a \emph{declarative} fashion.
This has the advantage of making the building of the image faster when modifying an already built recipe\ \cite{nxc}.
It also avoids the annoying optimization of the order of operations, frequent when building from a \texttt{Dockerfile}\ \cite{docker_cache}. 
As Nix packages are functions, introducing a variation means changing an argument when the function is called.

Systems like Debian store all the packages in the \texttt{/usr/bin} and \texttt{/usr/lib} directories.
This ordering can lead to conflicts between different versions of the same library, and it thus limits the introduction of variation in the environment without breaking the system.
On the other hand, Nix creates one directory per package.
Each directory name is prefixed by the hash of its sources.
Hence, if a user wants to install a different version of an already installed package, the sources will be different, thus the hash will be different, and Nix will then create a new directory to store the new package.
Those individual directories are stored in the \emph{Nix Store} located at \texttt{/nix/store}, in a read-only file-system.
The advantage of this fine-grained isolation method, is the \emph{precise} definition of the `\$PATH` environment variable to manage software environments.

The definition of packages through function also eases their sharing and distribution.
There is a large base of package definition done by the community, called \texttt{nixpkgs}\ \cite{nixpkgs}.
Users can easily base their new packages, or environment on those definitions.
It is also possible for independent teams and research groups to have their own base of packages.
Guix-HPC\ \cite{guix-hpc}, NUR-Kapack\ \cite{kapack}, or Ciment-channel\ \cite{ciment_channel} are examples of independent packages base for HPC and distributed systems.

A \textbf{\nix\ system profile} defines the configuration of the system (packages, \texttt{initrd}, etc.).
Among many features, a profile can define filesystems such as NFS and mount them automatically at boot time.
\nixos\ extend the ideas of \nix\ to the entire operating system.
A \nixos\ image can contain several profiles and \nix\ can switch between them by modifying symbolic links and restarting services via \texttt{systemd}.

\paragraph{Limits of Functional Package Managers}

Even though tools like Nix and Guix greatly improve the state of \repro\ for software environments, it is still possible to go wrong and make a package impure or to depend on an exterior state.
Nix is currently addressing this issue with the experimental feature \emph{Flake}\ \cite{flakes}.

To ensure the \repro\ and traceability of an environment, Nix requires that all the packages and their dependencies have their source code open and that the packages are packaged with Nix.
This could seem limiting in the case of proprietary software where the source code is unavailable (Intel compilers for example).
It is still possible to use such proprietary packages with the \texttt{impure} mode of Nix, but it breaks the traceability and thus the \repro\ of the software environment. 


The construction of the packages in a sandbox goes through an isolation mechanism of the file-system using \texttt{chroot}.
Historically, this feature is only available to users with \texttt{root} privileges.
But in the case of computing clusters, this kind of permissions greatly limits the adoption of Nix or Guix.
However, the \emph{unprivileged user namespace} feature of the Linux Kernel allows users to bypass this need of specific rights in most of the cases.

As Nix needs to recompile from source the packages are not available in its binary cache, it is possible that a future rebuild is impossible if the host of the source code disappear\ \cite{blinry}.
However, as Software Heritage now performs frequent archives of the open source repositories, it should be possible to find the sources of interest if needed.

These tools also require a change of point-of-view in the way of managing a software environment, which might make the learning curve intimidating.


\subsection{Workflow managers}\label{sec:sop:workflow}

\todo{more, make, et al}

Even if we did not record this information during the survey, a stricking realization is that almost no \ad\ made use of a workflow manager to run the experiments (maybe 1 out of all the  surveyed papers).
There are two main ways that the authors describe the workflow: very long \texttt{bash} scripts or a \texttt{README} file that require to copy-paste the commands.
Some commands are sometime directly included in the LaTeX, which makes it hard to read and to copy-paste.

As experiments in distributed computing can be quite expensive to run (especially if one needs access to a supercomputer or proprietary cloud), having the possibility to run a subset of the workflow is crucial.
For instance, a reviewer or future researcher might want to rerun only the analysis of the data from the artifact (dataset that has been stored on Zenodo), or maybe to add a new combination of parameters. 

Workflow managers \cite{wratten2021reproducible} such as Snakemake \cite{koster2012snakemake}, NextFlow \cite{di2017nextflow}, or Common Workflow Language\ \cite{amstutz2016common} based solutions (\eg\ Guix Workflow\ \cite{strozzi2019scalable}, or Toil\ \cite{vivian2017toil}) has become a standard in bioinformatics to run complex pipelines.

\begin{lesson}{Workflow managers}{}
  The workflows described in the artifacts either rely on manually copy-pasting commands from \texttt{README} files, or executing fragile \texttt{bash} scripts.
  The community would \emph{greatly} benefit by adopting workflow managers\ \cite{wratten2021reproducible}.
\end{lesson}

\subsection{Conclusion}\label{sec:sop:conclu}

From what we observed in this Section, we can conclude that the state-of-practice of \adae\ is not yet up to the state where it could be used for future researchers.

Proposition: the definition of a new badge assessing of the lifetime of \adae


%
% /////////////////////////////////////////////////////////////////////////////////////////////////////
\section{MAD: A novel metric for \ad}

We believe that there is the need to grade more precisely the \ad s of conference papers.
Based on the observations of Section\ \ref{sec:sop}, we propose the MAD (Metric for \ad) for grading the \ad s of authors.
The reproducibility committee can then select a threshold for allowing the "good" \ad s to be evaluated during the \aeval\ phase.
This would limit papers following bad reproducibility practices to waste the time and energy of reviewers.
\todo{also very hard to follow everything, thus a threshold}
\todo{give thresholds for ranks: A*, A, B}


\subsection{Source code}

\todo{set up points}

\begin{itemize}
  \item[0pt:] Software-Heritage
  \item[2pts:] Zenodo
  \item[4pts:] \texttt{git} URL with commit/tag
  \item[10pts:] unpinned \texttt{git} URL
  \item[30pts:] cloud URL
\end{itemize}


\subsection{Experimental platform}

\todo{set up points}

\begin{itemize}
  \item[0pt:] Testbeds
  \item[0pt:] Simulation
  \item[5pts:] Homemade
  \item[10pts:] Proprietary platforms
\end{itemize}

\subsection{Software environment}

\todo{set up points}

\begin{itemize}
  \item[0pt:] Nix/Guix
  \item[3pts:] Vendoring
  \item[7pts:] Virtual machine
  \item[8pts:] Container
\end{itemize}



\section{todo notes}

todo

testbeds cite lucas N

jupyter 

The current badges system covers a "\emph{short-term reproducibility}".

needs to protect the system against fake results 

The bare minimum is to provide access to the data and the analysis scripts to preform the data analysis.

The artifact review should not only make sure that the work presented in the paper is reproducible, but should also make sure that the work could be reused by others in the future.
This means that the sources, data, should be available, etc.

However, the artifact reviewing process is time consumming and painful.

We believe that by introducing a \ad\ form that can yield a grade, the artifact reviewers could quickly access which work has no chance of being reproducible in the long term.
The grade gives also the opportunity so conferences steering commities or artifact commities to defined their threshold for giving the usual badges.


We performed a study of N conferences of 2023 and looked at the \ad s of the accepted papers.

The contributions of this paper are:
\begin{itemize}
  \item a study of the state-of-practice of \ad\ in top conferences of computer science / HPC
  \item the definition of a metric to efficiently evaluate \ad s.
\end{itemize}


What is the future of \aeval?
should the \aeval\ be part of the accept/reject decision process?
What about the energy/environmental cost of \aeval?
especially in HPC where the experiments are long lasting and resources consumming.
If the current \aeval\ is the first step towards a more "important" reviewing process, the community should not get used to this level of rigourness for \aeval.


The use of testbeds for experiments.
Using platforms such as google cloud, microsoft azure, amazon aws should not be used.
Reproducibility goes hand to hand with open science and open source.
using proprietary tools (\eg\ Matlab) or platforms (\eg\ cloud providers) is a brake to reproducibility.


the report of the \aeval\ should be linked to the paper.
will be done in SC24

papers with code
rescience
joss

ai reproducibility?



%% PAPER ENDS HERE -----------------------------------------------------------------------------------------------

%\bibliographystyle{sty/ACM-Reference-Format}
%\bibliography{references}
\printbibliography

\end{document}
