\documentclass[sigconf,natbib=false]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or article.
\acmConference[ACM REP'24]{2024 ACM Conference on Reproducibility and Replicability}{June 18-20, 2024}{Rennes, France}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
\acmISBN{978-1-4503-XXXX-X/18/06}

\usepackage{tcolorbox}
\tcbuselibrary{theorems}
\usepackage{cleveref}
\newtcbtheorem[]{trap}{Pitfall}{colback=black!5,colframe=black!35,fonttitle=\bfseries}{th}
\newtcbtheorem[]{lesson}{Observation}{colback=black!5,colframe=black!35,fonttitle=\bfseries}{th}
\newcommand{\repro}{reproducibility}
\newcommand{\Repro}{Reproducibility}
\newcommand{\transpo}{\emph{Transposition}}
\newcommand{\flavour}{\emph{flavour}}
\newcommand{\flavours}{\emph{flavours}}
\newcommand{\ie}{\emph{i.e.,}}
\newcommand{\eg}{\emph{e.g.,}}
\newcommand{\nix}{\emph{Nix}}
\newcommand{\nixos}{\emph{NixOS}}
\newcommand{\nxc}{\emph{NixOS Compose}}
\newcommand{\enos}{\emph{EnOSlib}}
\newcommand{\grid}{\emph{Grid'5000}}
\newcommand{\kam}{\emph{Kameleon}}
\newcommand{\kad}{\emph{Kadeploy}}
\newcommand{\mel}{\emph{Melissa}}
\newcommand{\store}{\emph{Nix Store}}
\newcommand{\ad}{Artifact Description}
\newcommand{\aeval}{Artifact Evaluation}
\newcommand{\adae}{\ad/\aeval}
\newcommand{\todo}[1]{{\color{red}{TODO: #1}}}
\usepackage{hyperref}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{siunitx}
%\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{calc}
\usepackage{tabularx}
\usepackage{fontawesome}
\usepackage[para,online,flushleft]{threeparttable}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{amsmath,amsfonts}
\usepackage{textcomp}
\usepackage[inline]{enumitem}

\newcommand{\fmc}[1]{{\color{magenta} #1}} % Florina Ciorba


%\usepackage[backend=biber,style=trad-abbrv,firstinits=true]{biblatex}
\usepackage[
  datamodel=software,
  style=trad-abbrv,
  backend=biber
]{biblatex}
\addbibresource{references.bib}
\usepackage{software-biblatex}

\begin{document}

% \title{State-of-practice for sharing artifacts in top system conferences: Are~we~reproducible~yet?}
%\title{State-of-the-practice of artifacts longevity in system conferences}
\title{Longevity of Artifacts in Top Parallel and Distributed Systems Conferences: a~State-of-the-practice~Review}
%\title{Study of the State-of-the-practice for sharing artifacts in top system conferences from the point-of-view of artifacts longevity}
%\title{A longevity-focused review of the State-of-the-practice for sharing artifacts in top system conferences}

\author{Quentin Guilloteau}
\orcid{0009-0003-7645-5044}
\author{Florina M. Ciorba}
\orcid{0000-0002-2773-4499}
\email{firstname.lastname@unibas.ch}
\affiliation{%
  \institution{University of Basel}
  \city{Basel}
  \country{Switzerland}
}

\author{Millian Poquet}
\orcid{0000-0002-1368-5016}
\email{millian.poquet@irit.fr}
\affiliation{%
  \institution{Univ.~Toulouse,~CNRS,~IRIT}
  \city{Toulouse}
  \country{France}
}

\author{Dorian Goepp}
\orcid{https://orcid.org/0009-0007-3738-5919}
\author{Olivier Richard}
\orcid{https://orcid.org/0009-0005-8679-2874}
\email{firstname.lastname@inria.fr}
\affiliation{%
  \institution{Univ.~Grenoble~Alpes,~Inria,~CNRS,~LIG}
  \city{Grenoble}
  \country{France}
}

%\renewcommand{\shortauthors}{Guilloteau et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Reproducibility is the cornerstone of science. 
  Many scientific communities have been struck by the reproducibility crisis, and computer science is no exception.
  Its answer has been to require artifact evaluations along with accepted articles, and award badges to reward authors for their efforts to support 'reproducibility'.
  Authors can voluntarily submit artifacts associated with a submission to reviewers who decide their 'reproducibility' properties.
  We argue that the notion of 'reproducibility' considered by such badges is limited, and misses important aspects of the reproducibility crisis.
  In this article, we survey 296 articles from 5 top conferences in parallel and distributed systems held in 2023 (CCGrid, EuroSys, OSDI, PPoPP, and SC).
  For each article, we gather information about its artifacts (how it was shared, under which experimental setup, and how the software environment was generated and shared), as well as the reproducibility badges awarded.
  We found that the state-of-the-practice does not address reproducibility in terms of artifact \emph{longevity} and we expose 7 takeaways that support this finding.
  To address this issue, we propose a new badge based on source code, experimental setup, and software environment criteria, which will allow to reward artifacts that will withstand the test of time.% and support reproducibility in the long term.  
  This work aims to shed light on the issue of long-term reproducibility in parallel and distributed systems and to start a discussion in the community towards addressing the issue. 

  % The computer science community has also been struck by the reproducibility crisis\ \cite{baker500ScientistsLift2016, baker2016reproducibility}.
  % Its answer has been to set up artifact evaluations with a article acceptance, and award badges to reward authors for their "reproducibility" efforts.
  % The voluntary author can submit their artifacts to reviewers who will decide their "reproducibility" qualities.
  % However, the notion of "reproducibility" considered by the badges is limited, and it misses important aspects of the reproducibility problem.
  % In this article, we surveyed 296 articles from 5 top conferences in system and distributed systems of 2023.
  % For each article of these conferences, we gathered information about its artifacts (how it was shared, which experimental setup, and how the software environment was generated and shared), as well as the badges awarded.
  % We conclude that the state-of-the-practice does not address the problems of reproducibility in terms of \emph{longevity} of the artifacts.
  % To start a discussion with the community, we propose a new badge to reward artifacts that will resist the test of time, based on three criteria.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}
% 
% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002944.10011123.10010912</concept_id>
<concept_desc>General and reference~Empirical studies</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{General and reference~Empirical studies}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Reproducibility, Artifact Evaluation, Badges, Longevity}

\received{12 February 2024}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

\settopmatter{printfolios=true}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%% PAPER STARTS HERE -----------------------------------------------------------------------------------------------

\section{Introduction}

The scientific community as a whole is traversing a \repro\ crisis for the last decade.
Computer science is not exception to this crisis\ \cite{randallIrreproducibilityCrisisModern2018,baker500ScientistsLift2016}.
The \repro\ of research is essential to build solid knowledge and to increase reliability and confidence in the results, while limiting the methodology and analysis bias.
In 2015, Collberg et al.\ \cite{collberg_repeatability_2015} studied the \repro\ of 402 experimental articles published in \emph{system} conferences and journals of 2011 and 2012.
Each of the articles studied linked the source code used to perform their experiments. 
Of the 402 articles, 46\% were not reproducible.
The main causes were:
\emph{(i)} the source code was not available,
\emph{(ii)} the code did not compile or run,
\emph{(iii)} the experiments required specific hardware.

To reward authors of reproducible articles, several publishers, such as ACM or Springer, set up a peer review-based artifact evaluation for each submission.
This peer review process of the experimental artifact can yield one or several badges to the authors based on the level of \repro\ of their artifacts.

The term \repro\ is often used in a broad sense and gathers several concepts.
ACM proposed definitions for \repro\ terminology, that are used for the validation of the submitted artifacts\ \cite{acm-badges}.
%The definitions that we will use in the rest of this article are the ones the ACM uses for the validation of the submitted artifacts\ \cite{acm-badges}.
These definitions themselves rely on the International vocabulary of Metrology\ \cite{defs_vim}, and are composed of three levels summarized here:
\begin{enumerate*}[label=(\roman*)]
  \item \emph{Repeatable}: the measurements can be obtained again by the people at the origin of the work.
  %\item \emph{Repeatable}: the measurements (defined by \cite{defs_vim} as "the processes of experimentally obtaining or more quantity values that can be attributed to a quantity") can be obtained again by the people at the origin of the work.
\item \emph{Reproducible}: the measurements can be obtained again by people who do not belong to the original work and with the original artifact of the authors.
\item \emph{Replicable}: the measurements can be obtained again by people who do not belong to the original work with artifacts they develop independently.
\end{enumerate*}

% The evaluation of artifacts is a crucial point which allows guaranteeing the reproducibility of the experiments and the results.
% However, this reproducibility is not sufficient.
% Even if being able to reproduce an experiment is proof a scientific validation, the experiment and its environment are often too fragile to be extended by a third party, or even by the authors themselves. 

Reproducibility is harder to achieve when the artifacts of an experiment do not include the software environment in which it was conducted.
Indeed, side effects from the environment can happen and change the results of the experiment.
It is easy to forget to include an element in the software environment that has an impact on the performance of the experiment.
For instance, the performances, but also the outcome of a simple C application can depend on the compilation options\ \cite{stodden2018assessing} or also from the quantity of UNIX environment variables\ \cite{mytkowicz2009producing}.

Most of the current solutions in terms of '\repro' involve storing artifacts (system images, containers, virtual machines) and replay of experiments\ \cite{rosendo2020e2clab, brammer2011paper, brinckman2019computing}.
Even if this is an important step towards \repro, there is no guarantee that the software environment can be re-built in the future, and thus no guarantee that the experiments can be re-run if the artifacts disappear. 

The evaluation of artifacts for conferences is done soon after their initial construction.
Thus, it is very probable that the construction of the artifacts will be executed with package mirrors (\texttt{apt}, \texttt{rpm}, etc.) of a similar state or version.

This raises the question of: What will happen when someone tries to rebuild the artifacts' environment 1, 5, or 10 years into the future?
The objective of science is to be based on robust work to advance the frontiers of knowledge (\emph{Stand on the shoulders of giants} \cite{giant}).
Such '\textbf{short term reproducibility}' is a major obstacle to scientific progress and in complete opposition to Open Science \cite{openscience_unesco}.
No one would expect a mathematical proof to change through time or even completely disappear.
%\fmc{Which is? We need to cite this or define this better.}.
We believe that \ad\ mainly targets the artifact reviewer, \textbf{but, more importantly, it should target future readers of the article and future researchers.}

%The evaluation of artifacts for the conferences is done soon after their initial construction.
%It is thus very probable that the construction of the artifacts will be executed in a similar state of the packages mirrors (\texttt{apt}, \texttt{rpm}, etc.).
%However, what will happen when someone will try to rebuild the environment in 1 year? 5 years? 10 years?
%The objective of science is to base itself on robust works to continue to go forward (\emph{Stand on the shoulders of giants}).
%Such "\textbf{short term reproducibility}" is a major obstacle to scientific progress and is in complete opposition to the science philosophy.
%We believe that the \ad\ is for the artifact reviewer, \textbf{but, more importantly, for a future reader of the article.}


We believe that the concept that should be highlighted here is \textbf{variation}\ \cite{mercier2018considering, feitelson_repeatability_2015}.
This means allowing a third party to use the environment defined for an experiment to investigate another research idea.
An example of variation would be to change the MPI implementation used in an experiment (\eg\ MPICH instead of OpenMPI).
Being able to introduce such a variation requires the initial environment to be correctly defined.

However, even if variation is the end goal, we claim that the current state-of-the-practice of artifact description and evaluation does not yet fulfil the main reproducibility properties, in particular in terms of the reproducibility of the experimental software environment.

This work analyzes the \emph{longevity} of existing \ad s from the surveyed articles and exposes 7 observations messages summarizing our findings.
The \textit{key novelty} of this work is to propose a badge that complements existing reproducibility badges and supports the longevity of artifacts to withstand the test of time. 

This article is structured as follows.
Section \ref{sec:background} presents the context and work related to artifact evaluation.
%Section \ref{sec:reco} summarizes the recommendations made by the artifact reviewing committees.
In Section \ref{sec:sop}, we survey the articles from five top parallel and distributed systems conferences (CCGrid, EuroSys, OSDI, PPoPP, and SC) of 2023, and discuss the state-of-the-practice of artifact sharing.
%Based on our observations, we propose in Section \ref{sec:longevity} a new badge to take into account the \emph{longevity} dimension of an artifact.
Based on the findings of Section \ref{sec:sop}, in Section \ref{sec:longevity} we propose a new badge to take into account the \emph{longevity} aspect of an artifact.
Finally, Section \ref{sec:conclu} concludes the work and presents final remarks and perspectives.

%
% /////////////////////////////////////////////////////////////////////////////////////////////////////
\section{Background and related work}\label{sec:background}

The definitions proposed by ACM\ \cite{acm-badges} bear some confusion and the community did not reach a clear consensus \cite{plesser2018reproducibility, barba2018terminologies}.
In this paper, we modify the definition from \cite{rougier2019rescience} to add the dimension of the experimental platform:
"An experiment is \emph{reproducible} if the source code, the raw data, the analysis scripts are available, their usage sufficiently described for someone to re-perform the experiments and analysis, and the access to the used experimental platform is open".
An \emph{artifact} is a result of self-contained work with a context-specific purpose\ \cite{mendez2019artefacts}.

In 2015, Hunold\ \cite{hunold2015survey} conducted a survey among participants at the Euro-Par conference to assess the vision of the parallel computing community on reproducibility questions. 
When asked about the main reasons for not making the source code/raw data/data processing available, the participants answered that: "\emph{it is irrelevant because evolution is too fast}" (90\%), "\emph{it is not rewarding}" (87\%), "\emph{I want to retain a competitive advantage}" (84\%).
The second most popular answer is quite interesting, since the \aeval\ processes were not very popular at the time of the survey (the very first one was in 2011 at the ESC/FSE conference).
Since then, the sharing of artifacts and their evaluation has become an established and accepted practice with benefits for the community\ \cite{hermann2022has}.
The work by publishers with the badging system aimed to reward authors for sharing their artifacts.
It would be interesting to conduct the survey again today to quantify the impact of \aeval\ on this question, as we believe that reproducibility and its challenges have since gained greater visibility in terms of citations.
Badges have been shown to be an effective strategy to incentivize authors to make their research data available \cite{kidwell2016badges, rowhani2017incentives}.
However, the badges have not yet shown a significant impact on the visibility of the articles\ \cite{winter2022retrospective, frachtenberg2022research, heumuller2020publish}. 

In \cite{hermann2020community}, the authors surveyed the members of artifact evaluation committees of computer science conferences about their expectations for artifacts and the reviewing process of the artifacts. 
They found that despite the call for artifacts that expressed expected observable qualities from the submitted artifacts, there was no consensus on what the expected qualities should be.
The authors of \cite{castell2024towards} proposed a global "quality indicator" for research artifacts with a detailed framework, but it is not focused on reproducibility and does not integrate with the current badge system.
%Attempts have been made to define a global "quality indicator" of research artifacts\ \cite{castell2024towards}, but they have an orthogonal approach to the badge approach.
This lack of consensus leaves reviewers without guidelines to correctly and uniformly evaluate artifacts, which has been shown to be frustrating for reviewers\ \cite{beller2020will}.
In addition, the study showed that there is a lack of reviewer experience.

Reviewing and reusing artifacts require two different points-of-views.
Reviewing focuses more on the overall "quality" of the artifacts (\ie\ completeness, documentation), while the readers are more interested in their reusability.

An answer in the survey conducted in \cite{hermann2020community} explains that the experiments presented in a article should be reproducible, and that the good documentation and ease of setup are only bonuses.

Reusing artifacts poses problems when used to compare with other methods.
When researchers want to compare their new method with a method from the state-of-the-art, they either need to reimplement the method from scratch if no artifact is available, or, if the artifact is available, researchers need to adapt the code from the artifact in order to enable a comparison between the methods.
In both cases, this is not the original work that is being compared to, but a modified version of it, which might lead to different results.
An artifact with all the badges might not be reusable and comparable "as it is" by other researchers.
A solution that should be promoted by committees, is the implementation of the authors' solutions on collaborative benchmarking frameworks.
Some examples of such collaborative frameworks include BenchOpt \cite{moreau2022benchopt} for optimization problems, KheOps \cite{rosendo2023kheops} for Edge-to-Cloud experiments, or \cite{sharma2017towards} for networking experiments.



The survey participants also expressed that the most important thing is the availability of the artifact, rather than its reproducibility.



The last decade has seen the creation of independent online scientific journals to reward software and reproducibility.
The most popular example is probably the \emph{Journal of Open Source Software} (JOSS)\ \cite{smith2018journal} that publishes articles about open source \emph{research software}.
The review process, openly accessible as GitHub issues, includes a thorough inspection of a submission's source code, the documentation of the software, and a run-through of some examples.
%Reviews are open and hosted on GitHub as issues.
%IPOL \cite{colom2015ipol}
In the field of Image Processing, the online journal \emph{Image Processing On Line} (IPOL)\ \cite{colom2015ipol} requires the authors to implement the algorithms proposed in their article and to make the implementation available through an online demonstration for readers to explore and play with.
This requirement forces the authors to share their code alongside their article.
IPOL noted that this requirement also helped authors to improve their algorithms, as actually implementing the algorithms might raise some undetected edge-cases.
As the review process for journals is often much longer than for conferences, reviewers have more time to investigate the artifacts and iterate with the authors ways to improve the artifacts.


%Prova \cite{guerrera2019reproducible}
%UMLAUT \cite{umlaut}
%Benchmarking Crimes \cite{van2018benchmarking}
%Propagation of ML research  \cite{kang2023articles}
%Survey 4R \cite{hernandez2023repeatability}

Studies on the artifact process focus mainly on high-level characteristics, as well as on the availability and citations of the artifacts\ \cite{kidwell2016badges, rowhani2017incentives, winter2022retrospective, frachtenberg2022research, heumuller2020publish}. 
In this article, we propose a technical in-depth review of the methods and tools used for creating and sharing artifacts.


%
% /////////////////////////////////////////////////////////////////////////////////////////////////////
%\section{Recommendations for \ad s}\label{sec:reco}
%
%\todo{look at the recommendation of the conferences for the \adae, and blog posts, and summarize}

%
% /////////////////////////////////////////////////////////////////////////////////////////////////////
\section{State-of-the-practice in artifact sharing}\label{sec:sop}

\input{tables/summary_conferences}

In this section, we survey 296 articles from 5 of the top \emph{parallel and distributed systems} conferences of 2023, namely, CCGrid, EuroSys, OSDI, PPoPP, and SuperComputing (SC).
These conferences used an \adae\ process for the \emph{accepted} articles.
This \adae\ process usually consists of the authors writing an \ad\ as an appendix of the article to show how to get and use the artifact, how to install the dependencies, what are the different experiments and their estimated duration, etc. \cite{ae_tip, creating_successful_artifacts, ae_guidelines}.
The \ad\ section is typically one or two pages long (in a double column layout).
This \ad is complemented in practice by a link provided by the authors to a more detailed description of the artifact.
Table \ref{tab:table:paper_confs} summarizes our evaluations, including the number of articles for each conference and how many of them have an \ad.

% \begin{figure}
%   \centering
%   \includegraphics[width=0.5\textwidth]{figs/does_article_with_badge_has_artifact_section}
%   \caption{Number of articles which earned at least one badge which contain an \ad\ in the pdf. We observe that a significant amount of articles with at least a badge do not share their \ad\ in the proceedings version. \todo{remove plot and just put the percentage in the text}}\label{fig:}
% \end{figure}

The first surprising observation is that only about 20\% of the articles with the "Artifact available" badge do have an \ad~ section in the proceedings version.
We presume that the authors either forgot or declined to include this section in the final version of the article.
This is an unfortunate finding, as we believe that \textbf{the \ad\ is as valuable for the artifact reviewer as for the readers of the article.}
%\todo{put this claim somewhere else.}
%Excluding this section of the final version should not grant the "Open" badge.

\subsection{Methodology}

We selected conferences from 2023 with an \ad\ process and examined their published proceedings.
We surveyed all articles in the proceedings and for each article we noted:

\begin{itemize}
  \item How many reproducibility badges and which badges was the article awarded?
  \item Whether the article had an \ad\ section?
  \item Whether the article shared the URL of the artifact (it does not have to be in the \ad), and whether the URL is still valid?
  \item How the experiments were performed (\eg\ local machines, shared test-beds, proprietary machines, supercomputers, simulation, etc.)?
  \item How was the source code shared: \texttt{git} repository (\eg\ GitHub, GitLab), Zenodo, Software-Heritage, or combination of solutions?
  \item If the source code has been shared via \texttt{git} repository, we record the number of commits, and check whether a precise commit was specified by the authors
  \item How was the software environment shared?
\end{itemize}

The data presented in this article were collected \emph{manually by a single researcher}.
Although we did our best to correctly evaluate all the articles surveyed, there may be mistakes in the data.
%But we are certain that if there are errors, they will not affect significantly the conclusions of this article.
We spent between 5-10 minutes collecting the aforementioned information for each article.


In the following, we study four aspects of the \ad s: how was the source code shared (Section \ref{sec:sop:src}), where were the experiments executed (Section \ref{sec:sop:expe}), how was the software environment described and shared (Section \ref{sec:sop:sw}) and the workflow of the experiments (Section \ref{sec:sop:workflow}).

\subsection{Source code}\label{sec:sop:src}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figs/how_repo_shared}
  \caption{Methods used by the authors to share their artifacts (154 in total).
  The state-of-the-practice is dominated by \texttt{git} URLs, Zenodo archives, and a combination of the two (\texttt{git+zenodo}).}\label{fig:how_repo_shared}
\end{figure}

Figure \ref{fig:how_repo_shared} shows how the artifact was shared in the articles.
Note that some articles shared a link to their artifacts without having an artifact section or any badge.
Most articles simply include the URL of their \texttt{git} repository.
Some articles shared their code with Zenodo\ \cite{zenodo} or Figshare\ \cite{figshare}, and some shared with both a \texttt{git} URL and Zenodo.
A minority of articles used Software-Heritage\ \cite{swheritage} (abbreviated \texttt{swh} in Figure \ref{fig:how_repo_shared}), Globus\ \cite{globus}, or even personal cloud drives (\texttt{cloud}).

Some authors used \texttt{anonymous.4open.science}\ \cite{anonymous_github} (abbreviated \texttt{a4os} in Figure \ref{fig:how_repo_shared}) which allows users to share an anonymous copy of a public GitHub repository with reviewers.
This is particularly useful for double-blind reviews.
However, for all articles surveyed, all links to this service were dead and there was no way to retrieve the original \texttt{git} repository.
We believe that the links simply expired, which is problematic for reproducibility by future researchers.
Furthermore, as long as the \aeval\ process does not count in the accept/reject decision, having a double-blind review for the \aeval\ only limits communication between reviewers and authors.
If the results of the \aeval\ will be taken into account for the decision, then the community may need to investigate ways to perform the \aeval\ in a double-blind manner.
An easy solution would be to use tools such as \texttt{anonymous.4open.science} \cite{anonymous_github} for the review and then substitute the URL with a (more) persistent URL in the camera-ready version of the article.
Some communities use third parties to anonymously review artifacts, especially when they contain sensitive data (\eg\ \cite{perignon2019certify}).

A minority of authors shared their artifacts through an indirect link.
In most cases, this link points to the author's personal webpage, where there is the true link to access the artifact.
The drawback of this approach is that if the author's webpage is no longer accessible, the link given in the article is no longer valid.
Similar comments apply to the sharing of artifacts with a link to a personal cloud space (\eg\ Google Drive).

Sharing only with a \texttt{git} URL can lead to traceability issues.
For instance, only 6\% of the articles that shared artifacts via a \texttt{git} URL mentioned the commit used for the experiments.
Such a solution could be satisfactory for the \aeval\ since the delay between the submission of the article and the evaluation of its artifact is short enough for the source code to be unaltered or in a similar state.
However, for future researchers aiming to build upon these artifacts, it is nearly impossible to know which version of the code was used.
Another drawback of only using a \texttt{git} URL is that the source code hosted on forges (\eg\ GitHub, GitLab) might not be available forever.
For instance, authors could decide to delete or rename their repository, invalidating the URL given in the article.
A better solution would be to use an institutional account on the forges to store the \texttt{git} repository.
However, in the worst case, the entire source forge may need to close, making all repositories unavailable (\eg\ Google Cloud \cite{google_code}, GForge Inria).

One solution proposed by the conferences' reproducibility guidelines is to archive the code via Zenodo or Figshare, and then refer to the DOI generated by these archive websites in the \ad.
This has the advantage of giving a snapshot of the source code as it was at the time of submission and allows future use of the code.
However, storing source code on Zenodo has a simple drawback: There is no possibility for partial code exploration.
From the point of view of future researchers, having to download potentially large Zenodo archives to be able to explore a few source files may be cumbersome and may introduce friction.
% A better solution would be to explore the artifact via a simple web UI.
Archiving can also break the link between the original \texttt{git} URL if not archived correctly.
Zenodo has an integration with GitHub\ \cite{github_zenodo} that allows archiving \emph{releases} of a repository.
This is why some authors share both the \texttt{git} URL and a Zenodo archive.
If the link between the repository and the Zenodo archive breaks (\eg\ \texttt{git} repository becomes unavailable), future researchers are left with a single commit of the source code, and all the history of the project, which contributes to the understanding and extensibility of the project, is lost.
Some artifacts shared through Zenodo are actually archives of a \texttt{git} repository and include the \texttt{.git} folder, and thus the history of the project.
Zenodo and Figshare are adapted to archive datasets and binaries, not source code.
Zenodo and Figshare are heavily used because of the requirements from the artifact review committee to have well-identified and citable software, which goes through giving a DOI to the artifact.
However, these solutions are more appropriate for raw data and binaries, not for source code \cite{alliez2019attributing, software_heritage_2017}.

A more appropriate solution is to use Software-Heritage\ \cite{swheritage, di2017software}.
Similarly to Zenodo, it offers permanent storage of source code, with the same interface as usual source forges (\eg\ GitHub, GitLab, etc.).
This means that future researchers can explore the source code through an intuitive web interface without having to download any archive.
Software-Heritage also refers to the original source, so that future researchers can access it if still available.
%Authors give a unique identifier for the revision. 
For example, \cite{artefact-lifetime} is the Software-Heritage archive of this article repository.

%\begin{figure}
%  \centering
%  \includegraphics[width=0.5\textwidth]{figs/was_commit_fixed}
%  \caption{Number of articles that did precise an exact commit to use when the authors shared their source code \emph{only} via a \texttt{git} URL. We observe that the crushing majority do not precise the commit, and thus break the traceability of the artifact. \todo{remove plot and put percentage in the text}}\label{fig:was_commit_fixed}
%\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{./figs/number_commits_repo.pdf}
  \caption{Cumulative distribution function of the number of commits in the repositories shared in the articles}\label{fig:number_commits_repo}
\end{figure}

During this survey, we observed a surprising very low number of commits in the repositories linked in the articles when shared with \texttt{git} (\ie\ \texttt{git}, \texttt{git+zenodo}, \texttt{git+figshare} on Figure \ref{fig:how_repo_shared}).
Figure \ref{fig:number_commits_repo} shows the cumulative distribution function of the number of commits to the repositories shared in the articles when the source code was shared by \texttt{git}.
We can see that 25\% of the repositories have no more than 6 commits and that half of the repositories have less than 20 commits.
These repositories appear to be a "dump" of the source code with some extra commits for documentation.
Such practices do not allow reviewers and future researchers to explore the "true" history of the project, which is contrary to the Open Science principles of traceability\ \cite{openscience_unesco}.
The same comments as for a standalone Zenodo archive apply here.
It also casts doubt on the authors' good practices in terms of traceability of the experimentation.
We believe that this problem stems from the fact that the \aeval\ process and reproducibility may only be a second thought for certain authors.

\begin{lesson}{Sharing source code}{}
  The practice of sharing code through a \texttt{git} URL might result in the code becoming unavailable in the future.
  Archiving via Zenodo is better but may introduce friction for future exploration.
  Using Software-Heritage appears to be the best available solution to permanently share source code.
\end{lesson}

\subsection{Experimental setup}\label{sec:sop:expe}

An important point about reproducing experiments is the hardware used.
Any experiment that exhibits a particular behavior or performance evaluation should have a sufficiently detailed description of the hardware. 

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{./figs/experimental_setup.pdf}
  \caption{Experimental setup used in the survey articles.
  Most of the authors use local machines at their disposal.
  Some authors also use supercomputers to experiment on state-of-the-art systems.
  More concerning is the number of articles relying on proprietary platforms such as Amazon Web Services, Google Cloud, Microsoft Azure.
  Finally, a small proportion of the articles uses shared testbeds such as Chameleon, CloudLab, or Grid'5000.}\label{fig:experimental_setup}
\end{figure}

Figure \ref{fig:experimental_setup} shows the types of platforms on which the experiments were executed on for \emph{all} the surveyed articles, with or without \ad\ or badge.
Most of the experimental platforms were local machines (\texttt{local}), but the description of the machine (\ie\ CPU, GPU, disk, etc.) were given.
This still makes it difficult for reviewers and future researchers to find the exact hardware, or something closest to the used hardware.
In some \ad s, we observed that authors provided access to their local machines by giving the IP address and the password to connect.

A better solution would be to use open and shared platforms, also called \emph{testbeds}\ \cite{nussbaum2017testbeds}.
Chameleon\ \cite{chameleon}, Grid'5000\ \cite{grid5000}, or CloudLab \cite{cloudlab} are examples of such testbeds.
Testbeds are not frequently used (only about 5\% of the articles).
In practice, proprietary solutions (AWS, Azure, etc.) are used more frequently (15\% of the articles).
Authors rely on platforms such as Amazon Web Services, Microsoft Azure, Google Cloud, etc.
Even if this allows the reviewers to get an easier access to probably similar machines (in the short term), it locks the experiments, and thus their reproducibility, behind a paywall, which goes against the Open Science principles\ \cite{openscience_unesco}.
Using proprietary platforms also raises the question of who should pay to reproduce the results of the authors.
Some authors using such platforms wrote in their \ad\ the estimated monetary cost of rerunning the experiments.

Similarly, some articles (in particular for the SuperComputing conference (\texttt{SC})) used supercomputers to conduct experiments.
While supercomputers are at the bleeding edge of technology, having access to such a system is restrictive and can take several weeks or months before obtaining access.

\begin{lesson}{Experimenal setup}{}
  Most articles use machines that are difficult to access (local, supercomputer, or proprietary). 
  Testbeds are underrepresented in the state-of-the-practice, but appear to be better suited for reproducibility \cite{nussbaum2017testbeds}.
\end{lesson}

\subsection{Software environment}\label{sec:sop:sw}

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figs/sw_envs}
  \caption{Techniques used to share the software environment in the \ad s. Note than an article can use several of these techniques.}\label{fig:sw_envs}
\end{figure*}

\begin{figure*}
  \centering
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/how_packaged.pdf}
    \caption{Tools and technologies used by the authors to generate/package their software environment.
    Most artifacts used no tool.
    The others used virtualization tools (\eg\ containers or virtual machines), the most frequent being Docker.}\label{fig:techno}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figs/image_cache_bin.pdf}
    \caption{When authors used virtualization tools, most of them provided the image in a binary cache to spare the reviewers from building the image from source.
    However, only a small fraction of these images were archived in a long-term binary cache (always Zenodo).
    The most concerning observation is that we could not find the recipe of the image in about half of the artifacts.}\label{fig:cache_bin}
  \end{subfigure}
  \caption{Tools and technologies used to generate and package the software environment for the \aeval\ (Figure \ref{fig:techno}), and the state of the image and its recipe in the case of the use of virtual tools (Figure \ref{fig:cache_bin}).}\label{fig:techo_cache}
\end{figure*}


After downloading the correct version of the code on the correct platform, reviewers must configure the correct software environment to execute the experiments.
Figure \ref{fig:sw_envs} shows the different techniques used to describe and share the software environment in the \ad s.
Note that the authors can use \emph{several} of these techniques in their \ad.

In the following, we go through the methods observed to share the artifact software environment and discuss their reproducibility.

\subsubsection{Images}\label{sec:sop:sw:images}

Figure \ref{fig:techno} shows which tools were used to capture the software environment of the experiments.
Contrary to predictions made in 2017 \cite{silver2017software}, most of the \ad\ do not use any particular tool.
But some use virtualization tools, such as container or virtual machines.

Usually, the capture of the entire software stack goes through encapsulation in an \emph{image}.
This image can then be deployed on machines to execute the experiments.
A way to generate a system image is to start from a base image, deploy this image, execute the commands required to set up the desired environment, and finally compress the image.
Platforms such as \grid\ \cite{grid5000} and Chameleon\ \cite{chameleon} offer such tools to their users (respectively \texttt{tgz-g5k}\ \cite{tgz-g5k} and \texttt{cc-snapshot}\ \cite{cc-snapshot}).
In the context of repeatability and replicability, if the image stays available, then this way to produce system images is adequate at best.
But, concerning the traceability of the build, one cannot verify the commands that have been used to generate the image, and thus relies completely on the documentation from the experimenter.
Moreover, such images are not suited to be versioned with tools like \texttt{git} as they are in binary format.
In the situation where the image is no longer available, re-building the exact image is complex and the precise introduction of variation is impossible.

Figure \ref{fig:cache_bin} depicts the availability state of the images for \aeval.
We observed that most authors who use an image make it available in a binary cache such as DockerHub. 
However, DockerHub does not offer permanent storage of the images, or the authors could push another image on top of the previous one, thus losing the traceability for the experiments.
Another solution is to archive the image in a long-term binary cache, such as Zenodo.
However, it is not often done by the authors (only 12\% of the time).

A better approach to (re)generate and share images is with \emph{recipes}.
Those recipes, such as \texttt{Dockerfile}s for Docker containers or Kameleon recipes~\cite{ruiz_reconstructable_2015} for system images, are a sequence of commands to execute on a base image to generate the desired environment.
The text format of recipes makes them much more suitable to version, share, and reconstruct them.
The base images often have several versions, which are identified by labels called \emph{tags}.
In the case of Docker, the tag for the latest version is often called '\texttt{latest}'.
Basing an environment on this tag breaks the traceability and thus the reconstruction of the image itself.
Indeed, if a newer version is available at the time of a future rebuild of the environment, then the image will be based on this newer version and not the original version.
Another important question is to know whether the base image and all the versions can themselves be re-built, and if it is not the case, what is the permanence of the platforms hosting those images?
For instance, the longevity of the \texttt{nvidia/cuda} Docker image is only 6 months; after 6 months, the Nvidia administrators of DockerHub delete the images\ \cite{nvidia_cuda_lifetime}.
However, in Figure \ref{fig:cache_bin} we see that more than half of the \ad s using an image do not share the recipe, or we could not find the recipe to inspect or rebuild the image.
This means that if the image is not in a binary cache, then it is impossible to rebuild it exactly.
The column \texttt{Loose image} in Figure \ref{fig:sw_envs} shows the articles that based their software environments on an short \emph{longevity} image.


\subsubsection{List of package versions}\label{sec:sop:sw:list}

One of the popular approaches to share the software environment is simply to list the dependencies of the artifact.
We observed several levels to such this listing approach.
The first level is to only give the name of the dependencies (\texttt{List} in Figure \ref{fig:sw_envs}).
In this case, the reviewers or future researchers do not have information about the versions used or if there is any required feature from the dependencies.
Future versions of a dependency might have introduced breaking changes that you make the artifact unusable.
The authors can then give a minimum version to use(\texttt{List (>=)} in Figure \ref{fig:sw_envs}), for example \texttt{gcc >= 10.0.0}.
While this gives at least a lower bound on the versions, it does not prove that any future version would still work.
Finally, the most popular approach is to give for all dependencies the version used (\texttt{List (==)} in Figure \ref{fig:sw_envs}).
Listing all the dependencies by hand raises several important questions.
Are actually \emph{all} the dependencies listed?
What about the dependencies of the dependencies, etc.?
How do we get another system in the same state?

\begin{lesson}{Listing dependencies}{}
  Simply listing the packages used is not enough to regenerate the correct software environment.
\end{lesson}

\subsubsection{Package managers' installation commands}

Another popular way to describe the software environment is to list the installation commands in the package manager (\eg\ \texttt{apt}, \texttt{yum}).
These commands always looked like this:

\begin{verbatim}
sudo apt-get update
sudo apt-get install packageA packageB
\end{verbatim}

The question that arises is: What are the versions of the installed packages?
Indeed, calls to \texttt{apt-get update} (or equivalent for the other package managers) make the software environment depend on the state of the mirror of the package manager at the time the author did the experiments.
For the \aeval, the mirror may not change noticeably between the time of the experiments and the review.
However, there is a very low probability that in 5 or 10 years, the mirror will be in the same state, and the installed versions will be the exact same as for the experiments of the authors.
This approach also implicitly defines a dependency on the distribution of the operating system that needs to be used.

There are "workarounds" to make sure that the packages installed via classical package managers are the expected ones. 
One of them is to use a \emph{snapshot} of the mirror \cite{debian-snapshot}.
These snapshots are a dump of the mirror at a given time and users can then install packages from these snapshots using the usual interface of the package manager.
However, even using snapshots can cause issues.
In particular, what if the package installed from the snapshot creates a conflict with a package already installed on the system?
This is especially the case for systems based on the Filesystem Hierarchy Standard (FHS), such as Debian-based distributions, where all binaries and libraries are stored under \texttt{/usr/bin} and \texttt{/usr/lib}.
For example, what happens to the already installed packages if the artifact requires the installation of an old version of the \texttt{glibc}? 
One solution would be to use a virtualization tool such as a container or virtual machine, but, as seen in Section \ref{sec:sop:sw:images}, they have their own reproducibility issues.

Using snapshots makes it more difficult to introduce variation in the software environment.
Indeed, installing more recent packages might be tedious or introduce conflicts with the installed packages.


\begin{lesson}{Classical package managers}{}
  Installing dependencies through classical package managers (\eg\ \texttt{apt}, \texttt{yum}) creates a dependency on an uncontrollable state: the state of the mirror of the package manager.
  Freezing the state of the mirror introduces new compatibility problems with the underlying system and hinders the introduction of variation.
\end{lesson}

\subsubsection{\texttt{pip} and \texttt{conda}}

When the software environment contains only Python packages, freezing the dependencies with \texttt{pip} (\texttt{pip freeze}) is not enough.
\texttt{pip} only describes the Python environment, and ignores the system dependencies that numerous packages have. 
For example, freezing an environment containing the \texttt{zmq} Python package will not freeze the ZeroMQ system package installed on the system.  
Even if re-creating a Python environment from a \texttt{requirements.txt} is simple, installing a list of system packages with specific version is, on the other hand, much more complex.
In the best case, the repository includes a \texttt{requirements.txt} that lists all Python dependencies with the \emph{exact} versions.
However, in practice, we observed the same issues as presented in Section \ref{sec:sop:sw:list} when authors provide a list of dependencies without version or with a loose version.

\subsubsection{Downloading from the outside world}

A common practice when authors need to install a dependency that is not available through classical package managers is to install it from source.
For this, authors indicate in the \ad\ how to download the dependency and how to build it.
However, when cloning a \texttt{git} repository, a common error is to not specify the commit to use.
If no commit is specified, \texttt{git} will use the latest commit of the main branch, which could be completely different between the moment of the artifact review and 10 years later.
The same goes for archives downloaded via \texttt{wget}/\texttt{curl}, typically in the form
\begin{verbatim}
curl https://website.com/downloads/release-latest.tar.gz
\end{verbatim}
which outcome varies with time.
Both of these approaches were labeled \texttt{Imprecise download} in Figure \ref{fig:sw_envs}.


Moreover, the downloaded \texttt{git} repository could disappear in the future and, therefore, cloning from Software-Heritage would be more robust than cloning from a forge (\eg\ GitHub, GitLab).
The same remark applies to downloaded archives from websites.

Another important point is to check that the downloaded object is indeed the expected one.
This can be done by checking the cryptographic hash of the downloaded object and comparing it to the expected one.
Among all the articles surveyed, we observed this practice only once (\texttt{Verified download} in Figure \ref{fig:sw_envs}).

\begin{lesson}{Content of downloaded objects}{}
Every object coming from outside of the environment must be examined to ensure that it contains the expected content.
It is more preferable that the building of the environment fails if the content differs from the expected one, rather than the environment silently building with a different content.
\end{lesson}

\subsubsection{Modules}

A popular way to manage a software environment in HPC systems is through \emph{Modules} \cite{furlani1991modules, modules}.
Modules allow users to change their environment by "loading" and "unloading" packages and allow one to manage different versions of applications. 
Under the hood, modules change the \texttt{\$PATH} environment variables.
One drawback is that loading and unloading modules has side effects on the state of the system, and therefore might not reset the system to its initial state.
Manually loading the correct modules can also be quite error-prone for users.
However, modules are mainly maintained by the system administrators and are system-specific (\eg\ compile MPI with special optimizations for the underlying system).
Thus sharing a module-based environment between two systems might be impossible.
Modules are also helpful for administrators to limit and control the applications that can be run by users.
Moreover, as the modules are managed by the administrators, they do not have infinite longevity and might be unavailable in the future.
%\todo{Easybuild?}

\subsubsection{Spack}

Spack\ \cite{gamblin_spack_2015} is a package manager similar to \texttt{pip} but for all system packages and their dependencies.
It is possible to export the environment as a text file and rebuild it on another machine.
However, the produced environment might not be completely identical.
Indeed, Spack uses applications that are already present on the machine to build packages from the sources.
In particular, Spack assumes the presence of a C compiler on the system, and will use this C compiler to build the dependencies of the environment.
Hence, if two different machines have two different C compilers, then the resulting environment is likely to differ from the desired environment.
One clear advantage of Spack is the ease of introducing variation in an environment through the command line.
Spack can also be run as a non-privileged user and does not require the approval of the system administrators.
Spack will download and build dependencies into a folder located in the user's \texttt{\$HOME}.
However, a drawback of using Spack is that this directory consumes a lot of storage quota and inodes, which are limited on HPC systems.

\subsubsection{Vendoring}

One way to make sure to use the correct dependencies is to "vendor" them.
This means having a copy of the dependencies' source code in the artifact itself, and then build the dependencies from source.
Authors sometimes use \texttt{git submodules} to vendor.
However, \texttt{submodules} are not a copy of the dependencies, but simply a link to a specific commit of another \texttt{git} repository.
Hence, if one of the dependency's repository disappears, the artifact will not build.
Furthermore, the vendoring approach has its limits as it cannot reasonably capture all the dependencies by hand (\eg\ C compiler), so it is only limited to "close" dependencies.

\subsubsection{Functional package managers}

Tools such as Nix\ \cite{dolstra_nix_2004} or Guix\ \cite{courtes_functional_2013} fix most of the problems described in the previous sections.
However, they are barely used (about 1\% of the artifacts).
As Nix and Guix share similar concepts, in the following, we will focus on Nix, under the premise that all insights also apply to Guix.

Nix is a pure functional package manager for the reproducibility of the packages.
A Nix package is defined as a function where the dependencies of the packages represent the inputs of the function, and the body of the function contains the instructions to build the package.
Package building is done in a \emph{sandbox} which guarantees the build in a strict and controlled environment.
First, the sources are fetched, then their content verified by Nix.
If the hash of the sources differs from the expected hash, Nix stops the build of the package and yields an error.
Nix also fetches the dependencies recursively.
The build commands are then executed in the sandbox with the environment defined by the user.
At this stage, no network access or access to the local file system is possible.


Nix can generate environments that can be assimilated to a multi-language counterpart to Python's \texttt{virtualenv}s.
But it can also create containers images (Docker, Singularity, LXC, etc.), virtual machines, or full system images with the operating system NixOS.
The process of building an image with classical tools (\texttt{Dockerfile}, Kameleon recipe, etc.) is often iterative and arduous.
Defining an image with Nix is done in a \emph{declarative} fashion.
This has the advantage of making the image build faster when modifying an already built recipe\ \cite{nxc}.
It also avoids the tedious optimization of the order of operations, which is common when building from a \texttt{Dockerfile}\ \cite{docker_cache}. 
As Nix packages are functions, introducing a variation means changing an argument when the function is called.

Systems like Debian store all the packages in the \texttt{/usr/bin} and \texttt{/usr/lib} directories.
This ordering can lead to conflicts between different versions of the same library, and thus limits the introduction of variation in the environment without breaking the system.
Contrary to FHS-based systems, Nix installs each package in its own directory.
Each directory name is prefixed by the hash of its sources: \texttt{/nix/store/azvn85...-nix-2.18.1/}.
Hence, if a user wants to install a different version of an already installed package, its source code would be different, thus the hash will be different, and Nix will then create a new directory to store the new package.
These individual directories are stored in the \emph{Nix Store} located at \texttt{/nix/store}, in a \emph{read-only} file-system.
The advantage of this fine-grained isolation method is the \emph{precise} definition of the \texttt{\$PATH} environment variable to manage software environments.

The definition of packages through functions also facilitates their sharing and distribution.
There is a large base of package definitions written by the community and hosted in a \texttt{git} repository called \texttt{nixpkgs}\ \cite{nixpkgs}, and which is archived on Software-Heritage.
Users can easily base their new packages, or environment on those definitions.
It is also possible for independent teams and research groups to have their own base of packages.
Guix-HPC\ \cite{guix-hpc}, NUR-Kapack\ \cite{kapack}, or Ciment-channel\ \cite{ciment_channel} are examples of independent packages bases for HPC and distributed systems.

\paragraph{Limits of Functional Package Managers}

Even though tools like Nix and Guix greatly improve the state of \repro\ for software environments, it is still possible to go wrong and make a package impure or make it depend on some exterior state.
Nix is currently addressing this issue with the experimental feature \emph{Flake}\ \cite{flakes}.

To ensure the \repro\ and traceability of an environment, Nix requires that all the packages and their dependencies have their source code open and that the packages are packaged with Nix.
This could seem limiting in the case of proprietary software where the source code is unavailable (Intel compilers for example).
It is still possible to use such proprietary packages with the \texttt{impure} mode of Nix, but it breaks the traceability and thus the \repro\ of the software environment. 

The construction of the packages in a sandbox goes through an isolation mechanism of the file-system using \texttt{chroot}.
This feature used to be restricted to users with \texttt{root} privileges.
In the case of computing clusters, this kind of permissions greatly limits the adoption of Nix or Guix.
Thankfully, the \emph{unprivileged user namespace} feature of the Linux Kernel allows users to bypass this need of specific rights in most of the cases.

As Nix needs to recompile the packages that are not available in its binary cache from their source code, it is possible that a future rebuild is impossible if the host of the source code disappear\ \cite{blinry}.
However, as Software-Heritage now performs frequent archives of the open source repositories, it should be possible to find the sources of interest if needed.

Finally, these tools also require a change of point-of-view in the way of managing a software environment, which might make the learning curve intimidating.

\begin{lesson}{Functional package managers}{}
  Functional package managers (FPM), like Nix\ \cite{dolstra_nix_2004} or Guix\ \cite{courtes_functional_2013}, provide reproducibility guarantees on the produced software environment.
  However, FPMs are extremely underused, probably because of their steep learning curve.
  We believe that these tools are the closest to solving the reproducibility problems of software environment.
\end{lesson}

\subsection{Workflow managers}\label{sec:sop:workflow}

Even if we did not record this information during the survey, a striking realization is that almost no artifact made use of a workflow manager to run the experiments .
There are two main ways that the authors describe the workflow: lengthy and fragile \texttt{bash} scripts or a \texttt{README} file that require to copy-paste the commands.
Some commands are sometime directly included in the article itself, which makes it even harder to read and to copy-paste.

As experiments in distributed computing can be quite expensive to run (especially if one needs access to a supercomputer or proprietary cloud), having the possibility to run a subset of the workflow is crucial.
For instance, a reviewer or future researcher might want to rerun only the analysis of the data from the artifact (dataset that has been stored on Zenodo for example), or maybe to add a new combination of parameters. 

Workflow managers \cite{wratten2021reproducible} such as Snakemake \cite{koster2012snakemake}, NextFlow \cite{di2017nextflow}, or Common Workflow Language\ \cite{amstutz2016common} based solutions (\eg\ Guix Workflow\ \cite{strozzi2019scalable}, or Toil\ \cite{vivian2017toil}) have become a standard in bioinformatics to run complex pipelines.
However, their use has not yet reached the system and distributed systems communities, despite all the interesting qualities: robustness, scalability, interaction with the batch scheduler of a cluster\ \cite{snakemake-executor-plugin-slurm}.


\begin{lesson}{Workflow managers}{}
  The workflows described in the artifacts either rely on manually copy-pasting commands from \texttt{README} files, or executing fragile \texttt{bash} scripts.
  The community could \emph{greatly} benefit by adopting workflow managers\ \cite{wratten2021reproducible}.
\end{lesson}

% \subsection{Impressions after surveying the articles}\label{sec:sop:conclu}
% 
% \todo{give some high level feedback}

%
% /////////////////////////////////////////////////////////////////////////////////////////////////////
\section{Towards a new badge for artifacts}\label{sec:longevity}

%\begin{table*}
%  \caption{\label{tab:longevity}Proposition of grading framework for evaluating the \emph{longevity} of an artifact.}
%  \centering
%  \begin{tabular}[t]{l p{18em} p{9em} p{22em}}
%  \toprule
%    Grade & Source Code &  Experimental Setup & Software environment \\
%  \midrule
%    1/4 & Only \texttt{git} URL with a \emph{fixed} commit, or only Zenodo archive & Proprietary platforms & Vendoring or \emph{precise} download of dependencies \\
%    2/4 & \texttt{git} URL and Zenodo archive of a \emph{release} & Local machines & Docker/VM with recipe and long-term storage of the image \\
%    3/4 & \texttt{git} URL and Zenodo archive of the repository \emph{with} the history & Supercomputers & Spack \\
%    4/4 & Software-Heritage & Testbeds/Simulation  & Nix(OS) / Guix \\
%  \bottomrule
%  \end{tabular}
%\end{table*}

% \begin{table*}
%   \caption{\label{tab:longevity}Proposition of grading framework for evaluating the \emph{longevity} of an artifact.}
%   \centering
%   %\begin{tabular}[t]{l ll ll ll }
%     \begin{tabularx}{\textwidth}{l XX XX XX}
%   \toprule
%       \multirow{2}{2em}{Grade} & \multicolumn{2}{c}{Source Code} &  \multicolumn{2}{c}{Experimental Setup} & \multicolumn{2}{c}{Software environment} \\
%     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
%      &
%     Characteristics & Tools/Methods &
%     Characteristics & Tools/Methods &
%     Characteristics & Tools/Methods \\
%   \midrule
%     1/4 & fixed version, partial exploration & \texttt{git} URL with a \emph{fixed} commit & high monetary cost & Proprietary platforms & long-term, some dependencies & Vendoring, \emph{precise} download of dependencies \\
%     2/4 & long-term storage, fixed version & Archive of a \emph{release} & difficult access & Local machines & long-term, recipe, all dependencies, imprecise rebuild & Long-term storage of the image and recipe \\
%       3/4 & long-term storage, fixed version, history & Archive containing the history & longer-term, difficult access, low monetary cost & Supercomputers & shorter-term, recipe, most dependencies, more precise rebuild & Spack \\
%     4/4 & long-term storage, fixed version, history, partial exploration & Software-Heritage & longer-term, easy access, low monetary cost & Test-beds & long-term, recipe, exact rebuild, all dependencies & Nix(OS) / Guix \\
%   \bottomrule
%   \end{tabularx}
% \end{table*}


\begin{table*}
  \caption{\label{tab:longevity}Proposition of grading framework for evaluating the \emph{longevity} of an artifact.}
  \centering
    \begin{tabularx}{\textwidth}{l X X X}
  \toprule
      Grade & Source Code &  Experimental Setup & Software environment \\
  \midrule
      1/4 & fixed version, partial exploration (\eg\ \texttt{git} with \emph{fixed} commit) &  high monetary cost (\eg\ Proprietary platforms) & long-term, some dependencies (\eg\ Vendoring, \emph{precise} download) \\
       \midrule
      2/4 & long-term storage, fixed version (\eg\ Archive of a \emph{release}) & difficult access (\eg\ Local Machines) & shorter-term, recipe, most dependencies, more precise rebuild (\eg\ Spack) \\
       \midrule
      3/4 & long-term storage, fixed version, history (\eg\ Archive containing the history) & longer-term, difficult access, low monetary cost (\eg\ SuperComputer) & long-term, available recipe, all dependencies, imprecise rebuild (\eg\ Long-term storage of the image and recipe) \\
       \midrule
      4/4 & long-term storage, fixed version, history, partial exploration (\eg\ Software-Heritage) & longer-term, easy access, low monetary cost (\eg\ Test-beds) & long-term, recipe, exact rebuild, all dependencies (\eg\ Nix/Guix) \\
  \bottomrule
  \end{tabularx}
\end{table*}

We believe that the current badging system is missing one important aspect of the quality of the artifacts: their \emph{longevity}.
By \emph{longevity} we mean the time an artifact will be in the same state as the state used by the authors.
As we have seen in Section\ \ref{sec:sop}, the popular tools and methods to share source code, package software environment, or platforms to perform experiments on differ in their \emph{longevity} guarantees/quality.

Artifacts with long \emph{longevity} are much more valuable and impactful for future researchers to extend, and deserve to be rewarded and have more visibility.
Table\ \ref{tab:longevity} proposes a \textbf{first iteration} of a grading framework to evaluate the \emph{longevity} of an artifact based on three criteria: sharing of the source code, experimental setup used, and software environment.
We propose to grade each aspect on 4 levels going from insufficient (0/4) to best (4/4).
Performing the average of the grade for each criteria gives a global grade for the artifact.
We recommend for the global grade to be \emph{strictly} greater than 3/4 to deliver this new \emph{longevity} badge.
  The specifics of the grade for each criteria \textbf{should be further discussed in the community}, to reach a consensus on desired good practices.
They are also bound to change as the software tools and practices evolve.

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{figs/lifetime_score}
  \caption{Longevity score for each of the three criteria, and the global longevity score.
  We recommend a minimum longevity score of 3 to award the longevity badge.
  If we compute the longevity score on the survey articles of Section \ref{sec:sop}, then about 1 percent would have get the badge.}\label{fig:longevity_score}
\end{figure}

Figure \ref{fig:longevity_score} shows the distribution of scores for each of the three criteria (top three plots), and the global longevity score (bottom plot) for the articles surveyed in Section\ \ref{sec:sop}.
With the recommended threshold score of 3 out of 4, about 1 percent of the survey articles would have got the badge.
We can see that the most penalizing criteria is the software environment, where most articles have zero out of four, as they do not describe their software environment accurately enough to be stable through time.

%\todo{more discussion on the purpose of badges: rewarding effort, but it could also take a decision in the accept/reject process}



\section{Conclusion and perspectives}\label{sec:conclu}

\paragraph{Conclusion}

The badges given to the authors to reward their reproducibility efforts is a good way to encourage them to share their work and improve its quality.
However, the notion of "reproducibility" considered by the badges is limited, and it does not cover important aspects of the reproducibility crisis.
In Section \ref{sec:sop} of this article, we surveyed 296 articles from 5 top conferences in system and distributed systems (CCGrid, EuroSys, OSDI, PPoPP, SC) of 2023.
For each article of these conferences, we gathered information about its artifact and badges awarded.
We concluded that the state-of-the-practice does not address the problems of reproducibility in terms of \emph{longevity} of the artifacts.
Thus, in Section \ref{sec:longevity}, we proposed a new badge to reward artifacts that will resist the test of time.
We associate with this new badge, a framework for grading and delivering or not the badge to authors.
We hope that this new badge will be discussed and considered by the community, and adopted by conferences.%, with or without a scoring system.

\paragraph{Perspectives}

This study could be performed every year to assess the evolution through time of the community in terms of \emph{longevity} of artifacts.
We might want to expand the range of this study, and include dimensions such as workflow managers.
Collecting the data manually is slow and error prone.
Requiring artifacts to have a standard to describe the metadata of artifacts (similar to Software Bill of Material (SBOM) \cite{sbom, xia2023empirical}) would greatly improve artifact traceability.% (\eg\ for Nix \cite{genealogos}).

\paragraph{Threats to validity}

This study focuses only on 5 conferences on parallel and distributed system, and all from the \emph{same} year.
We believe that experiments in these fields are more complex than in less hardware dependent fields.
The fact that the data was collected by a single researcher might have introduced some bias.


% \todo{perspectives}
% 
% The artifact review should not only make sure that the work presented in the article is reproducible, but should also make sure that the work could be reused by others in the future.
% This means that the sources, data, should be available, etc.
% 
% the report of the \aeval\ should be linked to the article.
% will be done in SC24
% 
% What is the future of \aeval?
% should the \aeval\ be part of the accept/reject decision process?
% What about the energy/environmental cost of \aeval?
% especially in HPC where the experiments are long lasting and resources consuming.
% If the current \aeval\ is the first step towards a more "important" reviewing process, the community should not get used to this level of rigorousness for \aeval.
% 
% One perspective is to make even more apparent the lack of reproducibility of the popular methods to generate and package software environment.
% By collecting \texttt{Dockerfile}s from the artifacts of the articles, we could try to rebuild the Docker images from source periodically (\eg\ every month), and log the versions of the softwares in the resulting image.
% As \texttt{Dockerfile} recipes mostly rely on either nvidia or Ubuntu based images, calls to \texttt{apt}, and \texttt{pip}, the resulting software environment is very fragile, and would thus be interesting to follow its evolution. 
% This does not mean trying to rerun the experiments associated with the Docker images as it would be to energy consuming.
% So, even if the software environment varies, it does not mean that the results of the experiments will vary.

\section*{Acknowledgments}

This project received funding from the European Unions Horizon 2020 research and innovation programme under grant agreement No 957407 as DAPHNE.

%% PAPER ENDS HERE -----------------------------------------------------------------------------------------------

%\bibliographystyle{sty/ACM-Reference-Format}
%\bibliography{references}
\printbibliography

\end{document}
