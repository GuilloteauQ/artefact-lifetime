---
output:
  pdf_document:
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: template.tex
bibliography: references.bib
header-includes:
  - \usepackage{hyperref}
  - \usepackage{array}
  - \usepackage{caption}
  - \usepackage{graphicx}
  - \usepackage{siunitx}
  - \usepackage[table]{xcolor}
  - \usepackage{multirow}
  - \usepackage{hhline}
  - \usepackage{calc}
  - \usepackage{tabularx}
  - \usepackage{fontawesome}
  - \usepackage[para,online,flushleft]{threeparttable}
title: "A Pandoc Markdown Article Starter and Template"
author:
- name: Quentin Guilloteau
  email: "Quentin.Guilloteau[at]univ-grenoble-alpes.fr"
  institution: Univ. Grenoble Alpes, Inria, CNRS, LIG
  city: Grenoble
  country: France
abstract: "This document provides an introduction to R Markdown, argues for its benefits, and presents a sample manuscript template intended for an academic audience. I include basic syntax to R Markdown and a minimal working example of how the analysis itself can be conducted within R with the `knitr` package."
keywords: "pandoc, r markdown, knitr"
params:
    dataset: "../data/all.csv"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=FALSE,
                      message=FALSE, warning=FALSE,
                      echo=FALSE,
                      fig.path='figs/',
                      fig.align="center",
                      cache.path = '_cache/')
library(tidyverse)
theme_set(theme_bw() + theme(legend.position = "bottom"))

df <- read_csv(params$dataset, col_names = T)
df_unique <- df %>% group_by(doi) %>% top_n(1) %>% ungroup()
```

# Introduction

The scientific community as a whole has been traversing a \repro\ crisis for the last decade.
Computer science does not make an exception\ \cite{randallIrreproducibilityCrisisModern2018,baker500ScientistsLift2016}.

The \repro\ of the research work is essential to build robust knowledge, and it increases the reliability of results while limiting the number of methodology and analysis bias.
In 2015, Collberg et al.\ \cite{collberg_repeatability_2015} studied the \repro\ of 402 experimental papers published in *system* conferences and journals.
Each studied paper linked the source code used to perform their experiments. 
On those 402 papers, 46\% were not reproducible.
The main causes were:
*(i)* the source code was actually not available,
*(ii)* the code did not compile or did not run,
*(iii)* the experiments required specific hardware

To highlight the reproducible research works, several publishers (like ACM or Springer) set up an artifact evaluation of a submission.
This peer review process of the experimental artifact can yield one or several badgers to the authors based on the level of \repro\ of their artifacts.

The term \repro\ is often used in a broad sense and gathers several concepts.
The definitions that we will use in the rest of this thesis are the ones defined by ACM for the validation of the submitted artifacts\ \cite{acm-badges}.
It is composed of three levels of \repro:

1. *Repeatable*: the measures can be obtained again by the people at the origin of the work.

2. *Reproducible*: the measures can be obtained again by people who do not belong to the original work and with the original artifact of the authors.

3. *Replicable*: the measures can be obtained again by people who do not belong to the original work without the original artifact.

<!-- Nous utiliserons dans la suite la terminologie suivante\cite{feitelson_repeatability_2015, mercier2018considering}.
La **répétabilité** est le fait de relancer une expérience dans le même environnement que celui où elle a été conçue par les auteurs.
Si un tiers est capable d'executer l'experience dans l'environnement fournit par les auteurs et d'obtenir les mêmes conclusions, alors l'experience est **replicable**. -->

<!-- Un besoin de replicabilité serait l'évaluation d'artéfacts d'une soumission à une conférence par exemple. -->

The evaluation of artifact is a crucial point which allows to guarantee the reproducibility of the experiments and the results.
However, this reproducibility is not sufficient.
Even if being able to reproduce an experiment is proof a scientific validation, the experiment and its environment are often too rigid to be extended by a third party, or even by the authors themselves. 
We believe that the notion that should be pushed by the community is the *reproducibility with variation*.
By "variation" we mean that a third party is able to easily modify the environment of the experience to continue the research.
This means that the hardware and software environments as well as the experimental scripts must be correctly defined and can be modified easily.

This section focuses on the software environment.
For a global vision of the reproducibility problems, the readers might be interested in\ \cite{ivie2018reproducibility}.



Section\ \ref{sec:expe:repro:motiv} gives the context and the motivation of reproducibility for distributed systems.
Several frequently seen traps that might break the reproducibility of the software environment of an experiment are presented in Section\ \ref{sec:expe:repro:sw}.
Finally, Section\ \ref{sec:expe:repro:fpm} motivates the use of Functional Package Managers as a solution to most of the reproducibility problems of the software environments. 

<!---

# Environnement matériel {#sec:hw}

## Ne pas noter le matériel utilisé

Il est important de donner la description du matériel utilisé lors des expériences.
S'il s'agit par exemple d'une application MPI, les performances de cette dernière dépend aussi des CPUs et du réseau.

## Ne pas faire attention aux nœuds réservés

Bien que les clusters sont censés avoir la même configuration sur tous les nœuds, en pratique ce n'est pas le cas.
En effet, quelques disques peuvent avoir été changés ou avoir un partitionnement différent.

La topologie du réseau du cluster peut aussi avoir une influence sur les expériences, comme la position des nœuds réservés dans les différences VLANs par exemple. 


Grid'5000\ \cite{grid5000} a mis à disposition des wattmètres pour la consommation énergétique des machines.
Cependant, la consommation retournée par le wattmètre n'est pas celle d'un nœud, mais celle de la machine contenant plusieurs nœuds.
Ainsi pour connaitre la consommation d'un nœud, il faut bien réserver toute la machine et n'utiliser qu'un des nœuds.


Toutes les remarques précédentes peuvent être résolue avec une réservation bien précise des nœuds de calculs.


## Acces aux machines

TODO: c'est mieux si c'est facile d'avoir acces aux machines.

collberg et al.

distribution is missing files          12 19.4%
prerequisite failed to build            2 3.2%
unavailable environment                 3 4.8%
other errors                           15 24.2%
incomplete documentation               11 17.7%
missing third party package             8 12.9%
build:error comment[not needed,comment] 2 3.2%
runtime error                           9 14.5%

--->

## Context \& Motivation {#sec:expe:repro:motiv}

Imagine that in your childhood your grandma cooked a delicious chocolate cake, and that you now want to know how to do it yourself.
You even think that you can improve on it, and make it tastier!
You can try to reproduce the cake based on your far memories and culinary intuition, but the result might be disappointing...
Maybe your parents know the recipe!
But you might just get some fuzzy instructions.
Maybe your grandma just improvised the recipe!
Who knows?

You decide to go through the old stuff of your grandma's house.
By chance, you find an old recipe book.
You open it and from it falls a piece of paper with what looks like a cake recipe.
The handwriting matches the one from your grandma!

The recipe is clear, well detailed, and contains all the quantities for all the ingredients, the order of the different steps, the cooking time, etc.
You decide to follow the recipe literally, but the final result is not what you remembered...
Maybe you did not use the correct type of eggs, or that your oven is too different from your grandma's.
How to know?


An experiment without the environment in which it was executed makes it much more difficult to reproduce.
Indeed, side effects from the environment can happen and change the results of the experiment.
It is easy to forget to include in the software environment an element which impacts the performance of the experiment.
The performances, but also the results of a simple C application can depend on the compilation options\ \cite{stodden2018assessing} or also from the quantity of UNIX environment variables\ \cite{mytkowicz2009producing}.

Most of the current solutions in terms of "\repro" fall under the storage of artifacts (system images, containers, virtual machines) and replay of experiments\ \cite{rosendo2020e2clab, brammer2011paper, brinckman2019computing}.
Even if this is an important part of the \repro\ spectrum, nothing guarantees that the software environment can be re-built in the future, and thus nothing guarantees that the experiments can be re-run if the artifacts disappear. 

<!--
cela revient, dans notre exemple, à avoir la mixture déjà préparée par notre grand-mère et à mettre le gâteau soi-même dans son four à elle, qu'elle aura elle-même réglé.  
Comme les artéfacts sont évalués peu de temps après la mise en place de l'environnement, il est plus probable que l'évaluateur parvienne à le reconstruire, car l'état des miroirs de paquets (`apt`, `rpm`, etc) sont similaires.
--->

The step of artifact evaluation for the conferences is done soon after their initial construction.
It is thus very probable that the construction of the artifacts will be executed in a similar state of the packages mirrors (`apt`, `rpm`, etc.).
However, what will happen when someone will try to rebuild the environment in 1 year? 5 years? 10 years?
The objective of science is to base itself on robust works to continue to go forward (*Stand on the shoulders of giants*).
This vision of "*short term reproducibility*" is a major obstacle to scientific progress and is in complete opposition to the science philosophy.


We think that the notion that should be highlighted is the concept of **variation**\ \cite{mercier2018considering, feitelson_repeatability_2015}.
This means allowing a third party to use the environment defined for an experiment in order to investigate another research idea.
An example of variation would be to change the MPI implementation used in an experiment (\eg\ MPICH instead of OpenMPI).
Being able to introduce such a variation is only possible if the initial environment is correctly defined.


## Frequent Traps of the \repro\ of software environments {#sec:expe:repro:sw}


###### Sharing the Environment

An obvious way to fail the reproducibility of its experiments is not to share the used environments, or to share them in a perennial place.
Platforms such as Zenodo\ \cite{zenodo} or Software-Heritage\ \cite{swheritage} allow users to store artifacts (scripts, data, environments, etc.) permanently.


###### Knowledge of the environment

In the case where the software environment contains only Python packages, freezing the dependencies with `pip` (`pip freeze`) is not enough.
`pip` only describes the Python environment, and ignores the system dependencies that numerous packages have. 
For example, freezing an environment containing the `zmq` Python package will not freeze the ZeroMQ system package installed on the system.  
Even if re-creating a Python environment from a `requirements.txt` is simple, installing a list of system packages with specific version is on the other hand much more complex.
Moreover, listing manually all the system packages by hand is error-prone, and the best way to forget a package.

Tools such as Spack\ \cite{gamblin_spack_2015} have a similar approach as `pip` but also for all the system packages and their dependencies.
It is possible to export the environment as a text file and to rebuild it on another machine.
However, the produced environment might not be completely identical.
Indeed, Spack uses applications that are already present on the machine to build the packages from the sources.
Especially, Spack assumes the presence of a C compiler on the system, and will use this C compiler to build the dependencies of the environment.
Hence, if two different machines have two different C compiler then the resulting environment could differ from the desired environment.
One clear advantage of Spack is the ease to introduce a variation in an environment through the command line.


Solutions such as Spack, `pip`, `conda` only focus on the software stack above the operating system.
However, results from experiments might depend on the version of the kernel, some drivers, etc.
Thus, it is important to capture *entirely* the software stack.

\begin{trap}{Partially capturing the software environment of the experiment}{}
Experiments do not only depend on Python packages, but they can also depend on system packages, or even the version of the Linux kernel.
The software environment must be capture \emph{entirely}.
\end{trap}

Usually, the capture of the entire software stack goes through it encapsulation in a system image.
This image can then be deployed on machines to execute the experiments.
A way to generate a system image is to start from a base image, deploy this image, execute the commands required to set up the desired environment, and finally compress the image.
Platforms such as \grid\ \cite{grid5000} and Chamelon\ \cite{chameleon} propose to their users such tools (`tgz-g5k`\ \cite{tgz-g5k} and `cc-snapshot`\ \cite{cc-snapshot} respectively).
In the context of repeatability and replicability, if the image stays available then this way to produce system images is adequate at best.
But, concerning the traceability of the build, one cannot verify the commands that have been used to generate the image, and thus relies completely on the documentation from the experimenter.
Moreover, such images are not adapted to be versioned with tools like `git` as they are in a binary format.
In the situation where the image is no longer available, re-building the exact image is complex and the precise introduction of variation is utopian.

<!---
De maniere generale, simplement lister les paquets utilisés n'est pas suffisant car trop facile d'oublier des details.
La meilleure façon de partager un environnement logiciel est usuellement via une image qui est une *sandbox* pour les experiences.
Cette image peut être un conteneur, une machine virtuelle ou une image système.
### Créer une image sans recette

**TODO un mot sur debian like systems qui rendent difficile le fait d'avoir plusieurs versions du meme paquet ?**
--->

###### Depending on a uncontrollable state

<!---
## Actualiser le miroir lors de la construction de l'image
--->
<!---
## Prendre la derniere image en date (`latest`)
--->

One way to deploy a complete environment could be to rebuild it at each experiment.
EnOSlib\ \cite{cherrueau_enoslib_2022} is a Python library to manage distributed experiments.
It integrates a mechanism to install system packages in a programmatic fashion.
Users of EnOSlib can then, by executing their Python script, base their environment on a default image that they can modify as they need for their experiment.
This strategy of starting from scratch at every deployment has the advantage of making it harder to forget a dependency in the environment, as its absence would be detected at every execution.
However, it is still possible to make mistakes.
The \repro\ of the experiments using such solutions strongly depends on the base environment on which they are based.
These base environments are managed by administrators of the platforms and also have a finite lifetime, which raises the question of their permanence.  
Suppose that the administrators update the version of the base environment, what happens to the \repro\ of an experiment?

<!---
The development phase of the images for a distributed environment is an iterative and time-consuming process.
Reducing the duration of the development cycles goes towards improving the user experience, and we think that this is very important to reduce the friction to adopt reproducible experimental practices.
Tools such as \enos\ \cite{cherrueau_enoslib_2022} or Vagrant \cite{vagrant} go in this direction as they enable users to describe the configuration of the environment while abstracting the target platform.
\enos\ takes an \emph{imperative} approach by defining the environment with a pipeline of scripts that describes the provisioning phase and is executed at the beginning of an experiment to set up the desired software environment.
The target abstraction makes it easier to test the environment locally before deploying it at full scale.
However, \repro\ of the software stack is not a focus for these solutions, and they mostly just inherit \repro\ properties of the underlying platform and technologies.
Typically, one probably needs to combine \enos\ with tools such as \kam\ to make sure the kernel is reproducible, and the end user would be responsible for the \repro\ of the provisioning scripts.
To avoid depending on those base environment, it is possible to give a system image to EnOSlib.
EnOSlib will then deploy this image with Kadeploy\ \cite{kadeploy} as the starting point for the user experiment. 
This system image must then be available in a perennial fashion and rebuildable.
Moreover, it is also essential that solutions such as EnOSlib are correctly captured in the software environment.
--->

\begin{trap}{Forgetting to capture the software environment of the experiment/workflow manager}{}
Capturing the software environment in which the experiment/workflow manager is executed is as important and the environment of the experiment.
\end{trap}

A better approach to generate image is via *recipes*.
Those recipes, like `Dockerfile`s for Docker containers or Kameleon\ \cite{ruiz_reconstructable_2015} recipes for system images, are a sequence of commands to execute on a base image to generate the desired environment.
The text format of recipes make then much more suitable to version, share, and reconstruct them.
These base images have often several versions which are identified by labels called *tags*.
In the case of Docker, the tag of the latest version is often called `latest`.
Basing an environment on this tag breaks the traceability, and thus the reconstruction of the image itself.
Indeed, if a newer version is available at the time of a future rebuild of the environment, then the image will be based on this newer version and not the original version.
Another important question is to know if the base image and all the version are themselves reconstructive, and if it is not the case, what is the permanence of the platforms hosting those images?
For instance, the lifetime of `nvidia/cuda` Docker image is 6 months, after 6 months, the administrators delete the images.

\begin{trap}{Basing environments on non-reproducible images}{}
Be cautious with the longevity of the images on which you base your software environment.
By transitivity, if these images are not reproducible, so are yours.
\end{trap}

Another frequent problem is that the recipe performs an update of the mirror (\eg\ `apt get update`) before installing the required packages for the environment.
This has the bad property of breaking the \repro\ of the image.
Indeed, the image depends on the state of an external entity which is not controllable.
In order to be sure to use the exact same packages during a rebuild, a solution could be to use a *snapshot* of the mirror\footnote{Example for \texttt{debian}: \url{http://snapshot.debian.org/}}.
EnOSlib allows users to use this snapshot for the construction of the environment.
Concerning the introduction of variation, to base an image on a *snapshot* is quite constraining and can make impossible the installation of specific version of packages, or can create conflicts with already installed packages.

<!---
### Ne pas vérifier le contenu d'un téléchargement {#sec:sw:sha}
--->

When the recipe must download an object from the outside world, it is crucial to verify the content of the fetched object and compare it to the expected one.
In the case where the object is not checked, the environment depends on the state of the source of the object at the time of construction.
Hence, if the recipe calls `curl` to get a configuration file or a snapshot of a mirror for example, the recipe must also check the content of the files.
The usual way to do it is to compare the cryptographic hash of the downloaded object and the one of the expected object. 

<!---
### Ne pas vérifier le commit d'un code source
--->

A similar problem arises when the recipe downloads a package via `git` and build it from source.
In this case, it is paramount to correctly set the commit used in the recipe of the image.
Indeed, not knowing the commit used in the original image leads to having a dependence to the latest commit of the repository on the main branch.
Setting the commit used allows to know exactly the sources used, and simplifies the *controlled* introduction of variation in the environment (by changing commit for example). 

\begin{trap}{No checking the content of downloaded objects}{}
Every object coming from the outside of the environment must be examined to be sure that it contains the expected content. 
It is more important that the image fails to build if the content differs from the expected one, rather than the image silently builds with a different content.
\end{trap}

<!---

# Scripts d'expériences/analyse et execution {#sec:scripts}

**plus focus sur l'analyse**

Une fois l'environnement mis en place, il est temps de lancer l'expérience.
Ici aussi il y a une pléthore de moyens de casser la reproductibilité.
 
## Exécuter les expériences manuellement

Lancer les expériences manuellement est le meilleur moyen d'oublier d'exécuter une commande et met toute la pression sur la documentation.
Une meilleure idée est de scripter ses expériences.
Il peut être tentant de le faire avec de simples scripts shell, mais la difficulté à maintenir et faire évoluer est un frein pour la variation.
Des bibliothèques Python telles que Execo \cite{imbert_execo} ou EnOSlib \cite{cherrueau_enoslib_2022} permettent de scripter facilement des expériences sur des clusters de calculs en Python.
**faut packager ca aussi**

## Utiliser des notebooks

Les notebooks, comme Jupyter ou org-mode **REFS**, ont gagné en popularité avec Python.
Avec des bibliothèques, comme celles mentionnées plus haut, ces notebooks semblent être un excellent moyen de réaliser et partager des experiences.
Nous reprochons plusieurs choses à ces solutions.
Contrairement à un simple script, l'ordre d'exécution des cellules (ensemble de commandes) peut ne pas être linéaire.
De plus, les fichiers source des notebooks Jupyter ne sont pas adaptés au versionnage de par la sauvegarde des résultats dans le fichier même.
Dans le cas de org-mode, les notebooks sont souvent dépendants de la configuration Emacs de l'expérimentateur qui n'est pas encapsulé dans l'environnement.

## relancer entierement


## Ne pas versionner ses scripts 

## documentation des expes




## Des chemins absolus/relatifs en dur

**Repartir d'un env propre**
--->

## Functional Package Managers {#sec:expe:repro:fpm}


Tools such as Nix\ \cite{dolstra_nix_2004} or Guix\ \cite{courtes_functional_2013} fix most of the problems described in the previous section.
Nix and Guix share the similar concepts, in the following we will focus on Nix.

<!---
\begin{figure}
    \centering
    \includegraphics[width = 0.99\textwidth]{./figs/package-datamove.pdf}
    \caption{Comparison between traditional package managers and \nix. Traditional package managers fetch a built version of the package from a mirror, but information on how they have been built is unknown. In the case of \nix, the package is described as a \nix\ function that takes as input the source code and how to build it. If the package with these inputs has already been built and is available in the \nix\ caches (equivalents of mirrors) it is simply downloaded to the \store. Otherwise, it is built locally and added to the \store.}\label{fig:nix_packages}
\end{figure}

--->

Nix is a pure functional package manager for the reproducibility of the packages.
A Nix package is defined as a function where the dependencies of the packages are the inputs of the function, the body of the function contains the instructions to build the package.
The building of the packages is done in a *sandbox* which guarantees the build in a strict and controlled environment.
First, the sources are fetched, and the content verified by Nix.
If the hash of the sources differs from the expected hash, Nix stops the building of the package and yields an error.
Nix fetches also the dependencies and recursively.
The build commands are then executed in the sandbox with the environment defined by the user.
At this stage, no network access or access to the file system is possible.


Nix can generate environments that can be assimilated as multi-languages `virtualenv`s.
But it can also create containers images (Docker, Singularity, LXC, etc.), virtual machines, or full system images.
The process of building an image with classical tools (`Dockerfile`, Kameleon recipe, etc.) is often iterative and arduous.
Defining an image with Nix is done in a *declarative* fashion.
This has the advantage of making the building of the image faster when modifying an already built recipe\ \cite{nxc}.
It also avoids the annoying optimization of the order of operations, frequent when building from a `Dockerfile`\ \cite{docker_cache}. 
As Nix packages are functions, introducing a variation means changing an argument when the function is called.
<!---
In the listing\ \ref{chord}, it is easy to use another version of `simgrid` when calling the function to build the `chord` package.  
--->

Systems like `debian` store all the packages in the `/usr/bin` and `/usr/lib` directories.
This ordering can lead to conflicts between different versions of the same library, and it thus limits the introduction of variation in the environment without breaking the system.
On the other hand, Nix creates one directory per package.
Each directory name is prefixed by the hash of its sources.
Hence, if a user wants to install a different version of an already installed package, the sources will be different, thus the hash will be different, and Nix will then create a new directory to store the new package.
Those individual directories are stored in the *Nix Store* located at `/nix/store`, in a read-only file-system.
Figure \ref{fig:nix_packages} summarizes the differences between traditional package manager and \nix.
The advantage of this fine-grained isolation method, is the *precise* definition of the `$PATH` environment variable to manage software environments.


The definition of packages through function also eases their sharing and distribution.
There is a large base of package definition done by the community, called `nixpkgs`\ \cite{nixpkgs}.
Users can easily base their new packages, or environment on those definitions.
It is also possible for independent teams and research groups to have their own base of packages.
Guix-HPC\ \cite{guix-hpc}, NUR-Kapack\ \cite{kapack}, or Ciment-channel\ \cite{ciment_channel} are examples of independent packages base for HPC and distributed systems.

A \textbf{\nix\ system profile} defines the configuration of the system (packages, \texttt{initrd}, etc.).
Among many features, a profile can define filesystems such as NFS and mount them automatically at boot time.
Figure \ref{fig:nix_store} depicts an example of user profile containing the Batsim application \cite{dutot:hal-01333471}, which requires the SimGrid \cite{casanova:hal-01017319} library at runtime.
\nixos\ extend the ideas of \nix\ to the entire operating system.
A \nixos\ image can contain several profiles and \nix\ can switch between them by modifying symbolic links and restarting services via \texttt{systemd}.

<!---
\begin{figure}
    \centering
    \includegraphics[width = 0.9\textwidth]{./figs/store.pdf}
    \caption{Figuration of the \store\ content when the \texttt{alice} user has installed a Batsim \cite{dutot:hal-01333471} binary in her profile. As Batsim requires the SimGrid \cite{casanova:hal-01017319} library at runtime, SimGrid must also be in the store. Packages are stored in their own subdirectory, but common dependencies are not duplicated as symbolic links and shared libraries are used.
}\label{fig:nix_store}
\end{figure}

--->

## Limits of Functional Package Managers

Even though tools like Nix and Guix greatly improve the state of \repro\ for software environments, it is still possible to go wrong and make a package impure or to depend on an exterior state.
Nix is currently addressing this issue with the experimental feature *Flake*\ \cite{flakes}.

To ensure the \repro\ and traceability of an environment, Nix requires that all the packages and their dependencies have their source code open and that the packages are packaged with Nix.
This could seem limiting in the case of proprietary software where the source code is unavailable (Intel compilers for example).
It is still possible to use such proprietary packages with the `impure` mode of Nix, but it breaks the traceability and thus the \repro\ of the software environment. 



The construction of the packages in a sandbox goes through an isolation mechanism of the file-system using `chroot`.
Historically, this feature is only available to users with `root` privileges.
But in the case of computing clusters, this kind of permissions greatly limits the adoption of Nix or Guix.
However, the *unprivileged user namespace* feature of the Linux Kernel allows users to bypass this need of specific rights in most of the cases.

As Nix needs to recompile from source the packages are not available in its binary cache, it is possible that a future rebuild is impossible if the host of the source code disappear\ \cite{blinry}.
However, as Software Heritage now performs frequent archives of the open source repositories, it should be possible to find the sources of interest if needed.

<!---
L'utilisation de Nix ou Guix pourrait être opposée aux questionnements actuels autour de l'impact environnemental de l'informatique, car ils demandent de télécharger des quantités importantes de code source et de les recompiler sur les machines des utilisateurs.
Il existe des caches binaires pour les paquets populaires évitant ainsi la recompilation, mais leur durée de vie est limitée et nécessitera une recompilation pour reconstruire l'environnement dans plusieurs années. 
--->

These tools also require a change of point-of-view in the way of managing a software environment, which might make the learning curve intimidating.

## Conclusion

The computer science community starts to get interested in the problems of \repro\ of experiments.
However, the problems of \repro\ at the software level are not truly understood.
Setting up reproducible experiments is *extremely* complex.
The management of the software environment illustrate one facet of this complexity.
The usual tools (`pip`, `spack`, `docker`, etc.) do not answer the \repro\ problems without a huge effort by the experimenters, and only allow a \emph{short-term \repro}.
The *graal* of \repro\ is the precise introduction of variation in a third party defined environment.
This need for variation allows scientists to use solid contributions to continue research.
Even if there are no perfect solution yet, the need for a change of practice concerning \repro\ is needed.


# Study


Number of papers per conferences:

```{r }
per_conf <- df_unique %>%
  group_by(conference) %>%
  summarize(
    "Papers" = n(),
    "Found PDFs" = sum(pdf_available),
    "Repo URL" = sum(repo_url),
    "Dead URL" = sum(dead_url),
    "Artifact Section" = sum(artefact_section)
  )
total_conf <- per_conf %>%
  pivot_longer(!c("conference")) %>%
  group_by(name) %>%
  mutate(total = sum(value)) %>%
  select(-conference, -value) %>%
  pivot_longer(!c("name"), names_to = "conference", values_to = "value") %>%
  distinct(name, conference, value) %>%
  pivot_wider(id_cols=c("conference"), names_from = "name", values_from = "value")

per_conf %>%
  rbind(total_conf) %>%
  knitr::kable(format="latex", align="lccccc", caption="Papers considered by conferences.", booktabs = TRUE, linesep = c("",  "\\midrule"))
```


```{r, out.width="50%"}
df_unique %>%
  ggplot(aes(x = badges, fill=conference)) +
  geom_bar() +
  ggtitle("Distribution of the number of badges per paper") +
  xlab("Number of badges")
```

```{r, out.width="100%"}
df %>%
  filter(repo_url) %>%
  ggplot(aes(x = sw_env_method, fill = conference)) +
  geom_bar() + 
  theme(axis.text.x = element_text(angle = 45, hjust=1))
```

```{r, out.width="50%"}
df_unique %>%
  filter(repo_url) %>%
  ggplot(aes(x = techno, fill = conference)) +
  geom_bar() +
  xlab("") +
  ggtitle("Technology used to share the software environment")
```


```{r, out.width="50%"}
df_unique %>%
  filter(badges > 0) %>%
  ggplot(aes(x = artefact_section, fill = conference)) +
  geom_bar() +
  ggtitle("Does a paper with at least one badge has the `Artifact` section?") +
  xlab("")
```



```{r, out.width="50%"}
df_unique %>%
  filter(repo_url) %>%
  ggplot(aes(x = nb_commits_repo, fill = conference)) +
  geom_histogram() +
  xlab("") +
  ggtitle("Number of commits in the repositories")
```

```{r, out.width="50%"}
df_unique %>%
  filter(repo_url) %>%
  ggplot(aes(x = how_shared, fill = conference)) +
  geom_bar() +
  xlab("") +
  ggtitle("How was the repository shared?")
```

```{r, out.width="50%"}
df_unique %>%
  filter(repo_url & how_shared == "git") %>%
  ggplot(aes(x = pinned_version, fill = conference)) +
  geom_bar() +
  xlab("") +
  ggtitle("When shared with only `git`, was the commit used fixed?")
```


```{r, out.width="50%"}
df_unique %>%
  ggplot(aes(x = experimental_setup, fill = conference)) +
  geom_bar() +
  ggtitle("Experimental setup used") +
  xlab("")
```
